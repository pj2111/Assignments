{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeagvfs6A7PU"
      },
      "outputs": [],
      "source": [
        "# Extracting text from the doc and preprocessing using preprocessor from HS\n",
        "# Creating a prompt template\n",
        "# Loading a retriver and inmemory store\n",
        "# Created a , HS pipeline - using prompt node with mistral, retriver\n",
        "# Running RAG pipeline on the list of questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCMP5v36KELs",
        "outputId": "c5632243-9e63-4cf3-e12d-2bfc6ad2b125"
      },
      "outputs": [],
      "source": [
        "# !pip install farm-haystack[colab] PyPDF2 python-dotenv -q\n",
        "# !pip uninstall -y farm-haystack haystack-ai -q\n",
        "# !pip install haystack-ai -q\n",
        "!pip install PyPDF2 python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lh31eh-XFeoN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"]=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "N5F2Ha4HbLUq",
        "outputId": "fa435c7a-cb61-4b50-8e42-03f4a97e6d7f"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'PipelineSchemaError' from 'haystack.errors' (d:\\haystackenv\\Lib\\site-packages\\haystack\\errors.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Retriever \u001b[38;5;66;03m# requires farm-haystack\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/76031413/modulenotfounderror-no-module-named-haystack-nodes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_stores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocumentStore\n",
            "File \u001b[1;32md:\\haystackenv\\Lib\\site-packages\\haystack\\nodes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseComponent\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_classifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDocumentClassifier, TransformersDocumentClassifier\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnodes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntityExtractor, simplify_ner_for_qa\n",
            "File \u001b[1;32md:\\haystackenv\\Lib\\site-packages\\haystack\\nodes\\base.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document, MultiLabel\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PipelineSchemaError\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreflection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m args_to_kwargs\n\u001b[0;32m     15\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'PipelineSchemaError' from 'haystack.errors' (d:\\haystackenv\\Lib\\site-packages\\haystack\\errors.py)"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from pprint import pprint\n",
        "from getpass import getpass\n",
        "from haystack import Document\n",
        "from haystack import Pipeline\n",
        "from haystack.nodes import BM25Retriever # requires farm-haystack\n",
        "# https://stackoverflow.com/questions/76031413/modulenotfounderror-no-module-named-haystack-nodes\n",
        "# cannot import name 'PipelineSchemaError' from 'haystack.errors' \n",
        "# https://github.com/deepset-ai/haystack/issues/6771 informs farm-haystack is in maintenance mode\n",
        "# and requests to shift to haystack-ai 2.0 \n",
        "# review the migration guide here \n",
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "from haystack.nodes import PreProcessor, PromptModel, PromptTemplate, PromptNode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRZNcMHKbkQ-",
        "outputId": "980facf8-46eb-4d01-8627-a361609b9a80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Document: {'content': 'CUDA Thread Indexing Cheatsheet If you are a CUDA parallel programmer but sometimes you cannot wrap your head around thread indexing just like me then you are at the right place.  Many problems are naturally described in a flat, linear style mimicking our mental model of C’s memory layout. However, other tasks, especially those encountered in the computational sciences, are naturally embedded in two or three dimensions. For example, image processing tasks typically impose a regular 2D raster over the problem domain while computational fluid dynamics might be most naturally expressed by partitioning a volume over 3D grid.  \\n  NVIDIA CUDA Thread Model   \\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': '/content/CUDAThread-IndexingCheatsheet.pdf'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'db6624d7b773dcd2f6a93a97cf8d3c41'}>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_file_path = '/content/CUDAThread-IndexingCheatsheet.pdf'\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  text=''\n",
        "  with open(pdf_path,'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      text += page.extract_text()\n",
        "      return text\n",
        "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
        "doc = Document(content=pdf_text, meta={\"pdf_path\":pdf_file_path})\n",
        "docs = [doc]\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_THFyglc7Au",
        "outputId": "38defebe-af83-477e-980a-fab34b63057b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 522.26docs/s]\n"
          ]
        }
      ],
      "source": [
        "processor = PreProcessor(clean_whitespace = True,\n",
        "                         clean_header_footer = True,\n",
        "                         clean_empty_lines = True,\n",
        "                         split_by='word',\n",
        "                         split_length = 500,\n",
        "                         split_respect_sentence_boundary= True,\n",
        "                         split_overlap=0)\n",
        "\n",
        "processed_docs = processor.process(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GryzhThtWG1e",
        "outputId": "4b632580-8e64-4136-eda3-9b7de46dee37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(processed_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta86c8MfrWro",
        "outputId": "097f5103-254b-400b-a08c-8b8737a7e950"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Updating BM25 representation...: 100%|██████████| 1/1 [00:00<00:00, 1134.82 docs/s]\n"
          ]
        }
      ],
      "source": [
        "document_store = InMemoryDocumentStore(use_bm25=True)\n",
        "document_store.write_documents(processed_docs)\n",
        "retriver = BM25Retriever(document_store, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4pNJGWsgsS-P"
      },
      "outputs": [],
      "source": [
        "qa_template = PromptTemplate(prompt=\n",
        "      \"\"\"Using exclusively the information contained in the context, answer only the question asked without adding\n",
        "      suggestions for possible questions, and respond exclusively in english. If the answer can not be deduced from the\n",
        "      context ,\n",
        "      respond:\"Not sure because not relevant to the context\".\n",
        "      Context:{join(documents)};\n",
        "      Question:{query}\n",
        "      \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "NoDUbxwUWg_8",
        "outputId": "645b24c2-8b5d-49ee-8fcd-1feff6c4888f"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'haystack.components' has no attribute 'output_types'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-effcf8020e11>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# there is some import error in the haystack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ImportError: cannot import name 'default_from_dict' from 'haystack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# https://github.com/deepset-ai/haystack/issues/6470 tells restart session after pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# SPDX-License-Identifier: Apache-2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from haystack.components.generators.openai import (  # noqa: I001 (otherwise we end up with partial imports)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mOpenAIGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/openai.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mOpenAIGenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0mText\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0musing\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mlarge\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLLMs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/openai.py\u001b[0m in \u001b[0;36mOpenAIGenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_from_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'haystack.components' has no attribute 'output_types'"
          ]
        }
      ],
      "source": [
        "from haystack.components.generators.openai import OpenAIGenerator\n",
        "\n",
        "# there is some import error in the haystack\n",
        "# ImportError: cannot import name 'default_from_dict' from 'haystack\n",
        "# https://github.com/deepset-ai/haystack/issues/6470 tells restart session after pip install\n",
        "# module 'haystack.components' has no attribute 'output_types' is not found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "vjpM-v0DVS53",
        "outputId": "c292524d-284d-4f38-c28e-0fe9a1602a0a"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'component' from 'haystack' (/usr/local/lib/python3.10/dist-packages/haystack/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-74d7e8a3b877>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m llm = OpenAIGenerator(\n\u001b[1;32m      4\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GROQ_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mapi_base_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.groq.com/openai/v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# SPDX-License-Identifier: Apache-2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from haystack.components.generators.openai import (  # noqa: I001 (otherwise we end up with partial imports)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mOpenAIGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/openai.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatCompletion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatCompletionChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_from_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_to_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStreamingChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSecret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserialize_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserialize_secrets_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialize_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'component' from 'haystack' (/usr/local/lib/python3.10/dist-packages/haystack/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from haystack.components.generators import OpenAIGenerator\n",
        "\n",
        "llm = OpenAIGenerator(\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    api_base_url=\"https://api.groq.com/openai/v1\",\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    generation_kwargs = {\"max_tokens\": 512}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "UCU8aAQHD0FA",
        "outputId": "beafabb3-ad00-4aa9-ebe7-a31cc6bf8e21"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Model mixtral-8x7b-32768 is not supported - no matching invocation layer found. Currently supported invocation layers are: [<class 'haystack.nodes.prompt.invocation_layer.open_ai.OpenAIInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.azure_open_ai.AzureOpenAIInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.chatgpt.ChatGPTInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.azure_chatgpt.AzureChatGPTInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.anthropic_claude.AnthropicClaudeInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.cohere.CohereInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.hugging_face.HFLocalInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.hugging_face_inference.HFInferenceEndpointInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.aws_base.AWSBaseInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.amazon_bedrock.AmazonBedrockInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_base.SageMakerBaseInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_meta.SageMakerMetaInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_hf_infer.SageMakerHFInferenceInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_hf_text_gen.SageMakerHFTextGenerationInvocationLayer'>] You can implement and provide custom invocation layer for mixtral-8x7b-32768 by subclassing PromptModelInvocationLayer. Also please ensure you are authorised to load the model mixtral-8x7b-32768 and you are logged-in into the huggingface cli.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0243284509fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m prompt_node = PromptNode(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mapi_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.groq.com/openai/v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mixtral-8x7b-32768'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdefault_prompt_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqa_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36mwrapper_exportable_to_yaml\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Call the actual __init__ function with all the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_exportable_to_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/nodes/prompt/prompt_node.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, default_prompt_template, output_variable, max_length, api_key, api_base, timeout, use_auth_token, use_gpu, devices, stop_words, top_k, debug, model_kwargs, truncate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.prompt_model = PromptModel(\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36mwrapper_exportable_to_yaml\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Call the actual __init__ function with all the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_exportable_to_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/nodes/prompt/prompt_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_length, api_key, api_base, timeout, use_auth_token, use_gpu, devices, invocation_layer_class, model_kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_invocation_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_invocation_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_layer_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvocation_layer_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     def create_invocation_layer(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/nodes/prompt/prompt_model.py\u001b[0m in \u001b[0;36mcreate_invocation_layer\u001b[0;34m(self, invocation_layer_class)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 )\n\u001b[0;32m--> 117\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;34mf\"Model {self.model_name_or_path} is not supported - no matching invocation layer found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;34mf\" Currently supported invocation layers are: {PromptModelInvocationLayer.invocation_layer_providers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model mixtral-8x7b-32768 is not supported - no matching invocation layer found. Currently supported invocation layers are: [<class 'haystack.nodes.prompt.invocation_layer.open_ai.OpenAIInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.azure_open_ai.AzureOpenAIInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.chatgpt.ChatGPTInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.azure_chatgpt.AzureChatGPTInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.anthropic_claude.AnthropicClaudeInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.cohere.CohereInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.hugging_face.HFLocalInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.hugging_face_inference.HFInferenceEndpointInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.aws_base.AWSBaseInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.amazon_bedrock.AmazonBedrockInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_base.SageMakerBaseInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_meta.SageMakerMetaInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_hf_infer.SageMakerHFInferenceInvocationLayer'>, <class 'haystack.nodes.prompt.invocation_layer.sagemaker_hf_text_gen.SageMakerHFTextGenerationInvocationLayer'>] You can implement and provide custom invocation layer..."
          ]
        }
      ],
      "source": [
        "prompt_node = PromptNode(\n",
        "    api_base=\"https://api.groq.com/openai/v1\",\n",
        "    model_name_or_path='mixtral-8x7b-32768',\n",
        "    default_prompt_template=qa_template,\n",
        "    max_length = 500,\n",
        "    model_kwargs={\"model_max_length\":5000}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOhYHiQBL487"
      },
      "outputs": [],
      "source": [
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_node(component=retriver, name='retriver',inputs=['Query'])\n",
        "rag_pipeline.add_node(component=prompt_node,name='prompt_node',inputs=['retriver'])\n",
        "\n",
        "question_list = ['invoice number in the document', 'seller tax id in the document','seller address in the document',\n",
        "                 'client tax id in the document','IBAN no in the document']\n",
        "print_answer = lambda out:pprint(out['results'][0].strip())\n",
        "for question in question_list:\n",
        "  print_answer(rag_pipeline.run(query=question))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "zox54-LoqROo",
        "outputId": "81fa549f-52b0-4951-8cf8-a29868625a20"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'resolve_value'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f8badfbda34b>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n\u001b[1;32m     18\u001b[0m \u001b[0mprompt_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m llm = OpenAIGenerator(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGROQ_API_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mapi_base_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.groq.com/openai/v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/core/component/component.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mpre_init_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_COMPONENT_PRE_INIT_CALLBACK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_init_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mnamed_positional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComponentMeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_to_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/openai.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, model, streaming_callback, api_base_url, organization, system_prompt, generation_kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_base_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morganization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_base_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_telemetry_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'resolve_value'"
          ]
        }
      ],
      "source": [
        "from haystack import Pipeline\n",
        "# from haystack.utils import Secret\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "\n",
        "fetcher = LinkContentFetcher()\n",
        "converter = HTMLToDocument()\n",
        "prompt_template = \"\"\"\n",
        "According to the contents of this website:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "Answer the given question: {{query}}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt_builder = PromptBuilder(template=prompt_template)\n",
        "llm = OpenAIGenerator(\n",
        "    api_key=GROQ_API_KEY,\n",
        "    api_base_url=\"https://api.groq.com/openai/v1\",\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    generation_kwargs = {\"max_tokens\": 512}\n",
        ")\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(\"fetcher\", fetcher)\n",
        "pipeline.add_component(\"converter\", converter)\n",
        "pipeline.add_component(\"prompt\", prompt_builder)\n",
        "pipeline.add_component(\"llm\", llm)\n",
        "\n",
        "pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
        "pipeline.connect(\"converter.documents\", \"prompt.documents\")\n",
        "pipeline.connect(\"prompt.prompt\", \"llm.prompt\")\n",
        "\n",
        "result = pipeline.run({\"fetcher\": {\"urls\": [\"https://wow.groq.com/why-groq/\"]},\n",
        "              \"prompt\": {\"query\": \"Why should I use Groq for serving LLMs?\"}})\n",
        "\n",
        "print(result[\"llm\"][\"replies\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPSNJn5DqS2k",
        "outputId": "e05122e9-113b-400b-ee03-600ae8909d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.1.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.5/319.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boilerpy3 (from haystack-ai)\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting haystack-bm25 (from haystack-ai)\n",
            "  Downloading haystack_bm25-1.0.2-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.4)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from haystack-ai)\n",
            "  Downloading openai-1.30.5-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.0.3)\n",
            "Collecting posthog (from haystack-ai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.31.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2024.2.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.18.2)\n",
            "Installing collected packages: monotonic, lazy-imports, haystack-bm25, h11, boilerpy3, backoff, posthog, httpcore, httpx, openai, haystack-ai\n",
            "Successfully installed backoff-2.2.1 boilerpy3-1.0.7 h11-0.14.0 haystack-ai-2.1.2 haystack-bm25-1.0.2 httpcore-1.0.5 httpx-0.27.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.30.5 posthog-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install haystack-ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "5_zZpSNtqodX",
        "outputId": "6d202f5f-b154-4290-e480-4cc6dc441d78"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'resolve_value'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-413e8283c062>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSecret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m generator = OpenAIChatGenerator(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GROQ_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mapi_base_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.groq.com/openai/v1/models\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/core/component/component.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mpre_init_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_COMPONENT_PRE_INIT_CALLBACK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_init_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mnamed_positional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComponentMeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_to_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/components/generators/chat/openai.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, model, streaming_callback, api_base_url, organization, generation_kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_base_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morganization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_base_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_telemetry_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'resolve_value'"
          ]
        }
      ],
      "source": [
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.utils import Secret\n",
        "\n",
        "generator = OpenAIChatGenerator(\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    api_base_url=\"https://api.groq.com/openai/v1/models\",\n",
        "    model=\"llama3-8b-8192\",\n",
        "    generation_kwargs = {\"max_tokens\": 512}\n",
        ")\n",
        "\n",
        "\n",
        "messages = []\n",
        "\n",
        "while True:\n",
        "    msg = input(\"Enter your message or Q to exit\\n🧑 \")\n",
        "    if msg==\"Q\":\n",
        "        break\n",
        "    messages.append(ChatMessage.from_user(msg))\n",
        "    response = generator.run(messages=messages)\n",
        "    assistant_resp = response['replies'][0]\n",
        "    print(\"🤖 \"+assistant_resp.content)\n",
        "    messages.append(assistant_resp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epV5ZfLQ0nIQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeHbV8l4Fddf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
