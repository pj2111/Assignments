{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook dives into basic Text Classification fine-tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bringing in the Datasets\n",
    "    > Review the datasets, its features and names\n",
    "    If you are loading from csv, json or parquet ensure the \n",
    "    columns are clean, and you know the column names\n",
    "    > Need to work on the data, based on the task at hand. \n",
    "    (Need to complete other tutorial NBs in HF) \n",
    "    > Load the dataset based on the splits\n",
    "    > Create a Dataloader, Iterator out of the dataset\n",
    "\n",
    "- Bringing in the Tokenisers\n",
    "    > Decide on the type of tokenizer that best suits\n",
    "    > Practice creating new tokenizers and training them using own corpus\n",
    "    > Setup the function that tokenizes and returns the ids\n",
    "        + Review the padding, max_length, truncate options, review output\n",
    "\n",
    "- Preprocessing functions:\n",
    "    > Tokenise the input sequences, and remove the text data \n",
    "    > To process the input_ids for the task, write/ import the \n",
    "    functions, depending on the task \n",
    "    > Map the imported functions on the dataset, \n",
    "\n",
    "- Setup Training:  \n",
    "    > Instantiate the Training Arguments\n",
    "    > Instantiate DataCollators if required\n",
    "    > Instantiate the post-processing collator to support trainer\n",
    "    > Build the Trainer, with datasets and collators. \n",
    "    > Start the training\n",
    "\n",
    "- Work on Post Processing:\n",
    "    > Instantiate the metrics\n",
    "    > Write post-processing function for evaluation\n",
    "    > Run the evaluation and get the resuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First lets understand the datasets that are present for Text Classification. \n",
    "\n",
    "The most famous one is Glue. Which has many datasets.The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "CoLA (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a dataset containing sentences labeled grammatically correct or not.\n",
    "\n",
    "MNLI (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "\n",
    "MRPC (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
    "\n",
    "QNLI (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "\n",
    "QQP (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
    "\n",
    "RTE (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "\n",
    "SST-2 (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "\n",
    "STS-B (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "\n",
    "WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
    "\n",
    "Metrics are:\n",
    "\n",
    "for CoLA: Matthews Correlation Coefficient\n",
    "\n",
    "for MNLI (matched or mismatched): Accuracy\n",
    "\n",
    "for MRPC: Accuracy and F1 score\n",
    "\n",
    "for QNLI: Accuracy\n",
    "\n",
    "for QQP: Accuracy and F1 score\n",
    "\n",
    "for RTE: Accuracy\n",
    "\n",
    "for SST-2: Accuracy\n",
    "\n",
    "for STS-B: Pearson Correlation Coefficient and Spearman's_Rank_Correlation_Coefficient\n",
    "\n",
    "for WNLI: Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other datasets to work on TextClassification:\n",
    "\n",
    "Decided to check the Datasets page on HF, and looked at the most downloaded datasets.\n",
    "\n",
    "- IMDB (Already having it locally)\n",
    "\n",
    "- ccdv/arxiv-classification (2.1GB so dropping it due to its size)\n",
    "\n",
    "- ccdv/patent-classification (downloads to 285MB, so thinking of taking it)\n",
    "\n",
    "- jackhhao/jailbreak-classification (2MB, 2K rows)\n",
    "\n",
    "- knowledgator/events_classification_biotech (seems like a small 10~15MB dataset)\n",
    "\n",
    "Lets begin the process of pre-processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks = [\"cola\", \"mnli\", \"mnli-mm\",\n",
    "              \"mrpc\", \"qnli\",\"qqp\",\n",
    "              \"rte\", \"sst2\", \"stsb\",\n",
    "              \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"cola\"\n",
    "model_cp = \"distilbert-base-uncased\"  # Model used for classification \n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset for a task, and its corresponding metric\n",
    "\n",
    "actual_task = \"mnli\" if task == 'mnli-mm' else task\n",
    "\n",
    "dataset = load_dataset('glue', actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_refs = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds,\n",
    "              references=fake_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_cp)  # will pull only the tokenizer\n",
    "tokenizer(\"this is 1st sentence\", \"followed by spidey sense of tropical tree top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set can be used for further classification training by minor modification, \n",
    "# so the below task to keys are created.\n",
    "\n",
    "task_to_keys = {\n",
    "    glue_tasks[0]:(\"sentence\", None),\n",
    "    glue_tasks[1]:(\"premise\", \"hypothesis\"),\n",
    "    glue_tasks[2]:(\"premise\", \"hypothesis\"),\n",
    "    glue_tasks[3]:(\"sentence1\", \"sentence2\"),\n",
    "    glue_tasks[4]:(\"question\", \"sentence\"),\n",
    "    glue_tasks[5]:(\"question1\", \"question2\"),\n",
    "    glue_tasks[6]:(\"sentence1\", \"sentence2\"),\n",
    "    glue_tasks[7]:(\"sentence\", None),\n",
    "    glue_tasks[8]:(\"sentence1\", \"sentence2\"),\n",
    "    glue_tasks[9]:(\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Sentence2: {dataset['train'][0][sentence2_key]}\")\n",
    "    print(f\"Sentence1: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Function that tokenizes based on the type of task, \n",
    "    and truncates the sentence that is longer than the model \n",
    "    can handle\"\"\"\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], truncation=True),  tokenizer(examples[sentence2_key], truncation=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_ds = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "num_labels = 3 if task.startswith('mnli') else 1 if task == 'sstb' else 2\n",
    "# task is mnle then 3, if task is sstb then 1, rest of cases 2\n",
    "num_labels  # expecting 2 as it is Cola task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_cp,\n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == 'sstb' else 'matthews_correlation' \\\n",
    "            if task == 'cola' else \"accuracy\"\n",
    "metric_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer here is basically a wrapper function around the training loop that we \n",
    "# created using Torch, Tensors and Wine_dataset\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"/home/kamal/training_files/prac/\",\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=2,\n",
    "    # save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=metric,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    hub_model_id=f\"kamaljp/{model_cp}-finetuned-{task}\",\n",
    "    skip_memory_metrics=True  # this is for avoiding the threadlock error\n",
    "    # https://github.com/huggingface/transformers/issues/17696\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(eval_pred):\n",
    "    pred, refs = eval_pred\n",
    "    if task != 'sstb':\n",
    "        predictions = np.argmax(pred, axis=1)\n",
    "    else:\n",
    "        predictions = pred[:, 0]\n",
    "    \n",
    "    return metric.compute(predictions=predictions,\n",
    "                         references=refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" \\\n",
    "                if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenised_ds['train'],\n",
    "    eval_dataset=tokenised_ds[validation_key],\n",
    "    compute_metrics=compute_metric,\n",
    "    tokenizer=tokenizer, # this is new, and it is required for padding,\n",
    ")\n",
    "# if no tokenizer is provided then length mismatch occurs, leading to error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using evaluator\n",
    "from evaluate import evaluator\n",
    "\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=model,\n",
    "    data=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},\n",
    "    strategy=\"bootstrap\",\n",
    "    n_resamples=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
