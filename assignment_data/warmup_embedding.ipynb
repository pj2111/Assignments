{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301602d4-a0df-407e-afa1-ccfdff046332",
   "metadata": {},
   "source": [
    "Notebook introduces the Embeddings and their use cases \n",
    "\n",
    "It all starts with Data, in this case the data is taken from the https://faq.ssa.gov/en-US/\n",
    "\n",
    "Note, Embedding is a process of converting a word or a number into a vector of certain dimensions\n",
    "Tokenizer and Embedding models are not same. They are different. \n",
    "\n",
    "Tokenizers are functions written in python that take a corpus of data and returns a dictionary-id map. Based on which the tokenizers, work on the sentences.\n",
    "\n",
    "Embedding models are Neural Networks coded in Torch/TF/Jax/Flax that are used for creating vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d81148-d9f3-48a0-b185-d37a0943e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to work with sentence transformers library. \n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_embedding = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b2d8cc7-e5d2-4804-901d-669358cd58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"How do I get a replacement Medicare card?\",\n",
    "        \"What is the monthly premium for Medicare Part B?\",\n",
    "        \"How do I terminate my Medicare Part B (medical insurance)?\",\n",
    "        \"How do I sign up for Medicare?\",\n",
    "        \"Can I sign up for Medicare Part B if I am working and have health insurance through an employer?\",\n",
    "        \"How do I sign up for Medicare Part B if I already have Part A?\",\n",
    "        \"What are Medicare late enrollment penalties?\",\n",
    "        \"What is Medicare and who can get it?\",\n",
    "        \"How can I get help with my Medicare Part A and Part B premiums?\",\n",
    "        \"What are the different parts of Medicare?\",\n",
    "        \"Will my Medicare premiums be higher because of my higher income?\",\n",
    "        \"What is TRICARE ?\",\n",
    "        \"Should I sign up for Medicare Part B if I have Veterans' Benefits?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home work\n",
    "# load a paragraph - chunk into word tokens- use anything(Autotokenizer or manually using str.split(' '))\n",
    "# stitch into 10 word sentences\n",
    "# create function for reuse\n",
    "\n",
    "# convert to use chunk size and chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc99ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "How to do this using autotokenizer? #Doubt- sol - reverse get_vocab and try look for a method that does same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68246d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c693dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City,\n",
    "originally as a company that developed a chatbot app targeted at teenagers.[1] The company was named after the hugging face \n",
    "emoji.[1] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.\"\"\"\n",
    "para_list = paragraph.split(\" \")\n",
    "len(para_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcb4ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.[1] The company was named after the hugging face emoji.[1] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The company was founded in 2016 by French entrepreneurs Clément',\n",
       " 'Delangue, Julien Chaumond, and Thomas Wolf in New York City,',\n",
       " 'originally as a company that developed a chatbot app targeted',\n",
       " 'at teenagers.[1] The company was named after the hugging face',\n",
       " 'emoji.[1] After open sourcing the model behind the chatbot, the',\n",
       " 'company pivoted to focus on being a platform for machine',\n",
       " 'learning.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_10_words =[]\n",
    "\n",
    "for i in range(0,len(para_list),10):\n",
    "    print(i)\n",
    "    sent_10_words.append(\" \".join(para_list[i:i+10]))\n",
    "print(paragraph)\n",
    "sent_10_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "224e5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_join_10_word_sent(text):\n",
    "    text_list = text.split(\" \")\n",
    "    l1=[]\n",
    "    for i in range(0,len(text_list), 10):\n",
    "        l1.append(\" \".join(text_list[i:i+10]))\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "266e69fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The company was founded in 2016 by French entrepreneurs Clément',\n",
       " 'Delangue, Julien Chaumond, and Thomas Wolf in New York City,',\n",
       " 'originally as a company that developed a chatbot app targeted',\n",
       " 'at teenagers.[1] The company was named after the hugging face',\n",
       " 'emoji.[1] After open sourcing the model behind the chatbot, the',\n",
       " 'company pivoted to focus on being a platform for machine',\n",
       " 'learning.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_and_join_10_word_sent(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01d477f-5ac3-4d1e-97ba-0a28a6f705e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_embedding.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "650429f5-8fc3-4359-9bcb-2439be375d6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m texts_tokenized \u001b[39m=\u001b[39m model_embedding\u001b[39m.\u001b[39mtokenize(texts\u001b[39m=\u001b[39mtexts[\u001b[39m0\u001b[39m],)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "texts_tokenized = model_embedding.tokenize(texts=texts[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60512292-b734-4458-aa7b-3f59b3b01ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb98f5-8e9c-4158-ba7c-896aff4084f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3889e-02,  5.5259e-02, -1.1655e-02, -3.3414e-02, -1.2261e-02,\n",
       "        -2.4873e-02, -1.2663e-02,  2.5346e-02,  1.8508e-02, -8.3508e-02,\n",
       "        -9.3020e-02,  1.4486e-02, -1.7411e-02, -8.8344e-02, -4.4791e-03,\n",
       "        -4.6326e-02, -1.3194e-02,  3.5382e-02,  6.2311e-02,  4.8590e-02,\n",
       "        -5.9118e-02,  5.4135e-02, -6.4397e-02,  3.4024e-02,  6.6364e-03,\n",
       "         3.5917e-02, -6.7838e-02, -1.7735e-02, -1.2722e-02,  4.6462e-02,\n",
       "         1.0864e-01,  2.3821e-02, -2.6996e-02,  3.7174e-02,  9.7598e-02,\n",
       "        -2.7030e-02, -4.5430e-02,  3.1817e-02, -3.3746e-02, -1.5199e-02,\n",
       "        -2.1536e-02,  1.4811e-02, -2.0892e-02,  6.8857e-02,  5.0174e-02,\n",
       "        -2.4728e-02, -6.2768e-02,  4.8287e-02,  8.2910e-02,  7.9220e-02,\n",
       "        -4.2627e-02, -4.3972e-02, -1.8923e-02,  7.5505e-02,  5.2448e-03,\n",
       "         8.9569e-03,  1.3388e-02,  1.3386e-02, -3.7899e-02, -2.7515e-03,\n",
       "        -9.3992e-03,  1.0537e-02,  1.3885e-02,  1.0505e-01, -1.1193e-01,\n",
       "         3.5918e-02,  1.0220e-01, -7.2759e-03,  6.9250e-02, -2.4544e-02,\n",
       "        -2.5699e-02, -2.2103e-02, -2.3406e-02,  1.3708e-02,  4.0491e-02,\n",
       "         1.4141e-02,  2.4133e-02, -6.5803e-04,  4.5965e-02,  1.7059e-02,\n",
       "        -9.3535e-02, -5.3271e-02,  1.8538e-02,  7.2048e-02,  1.0834e-01,\n",
       "         1.8587e-02, -3.5637e-02,  3.0584e-02, -7.8270e-03, -1.4045e-01,\n",
       "         3.1435e-02,  7.6295e-02, -6.3820e-02, -6.5556e-02, -3.1062e-02,\n",
       "        -6.2086e-02,  2.9290e-02, -5.9932e-02, -3.4273e-02,  7.2154e-02,\n",
       "         2.5475e-02, -5.4016e-02,  6.5119e-02, -3.6695e-02,  1.1209e-02,\n",
       "        -1.3093e-02,  5.3316e-02,  1.1083e-01,  1.7506e-02, -2.1824e-02,\n",
       "        -4.0172e-03, -1.7914e-02,  3.7712e-02,  9.8195e-03, -6.2193e-02,\n",
       "         6.2280e-02,  4.7749e-03, -1.0210e-02,  6.4433e-02,  2.9476e-02,\n",
       "        -4.1480e-02, -3.9276e-02, -3.8645e-02, -1.1428e-02, -6.9637e-02,\n",
       "         5.6948e-02, -7.1699e-03, -4.9244e-33,  7.1862e-02,  4.2734e-02,\n",
       "         1.8728e-02, -5.0314e-02,  3.9314e-02,  5.1541e-03,  9.4651e-02,\n",
       "        -8.9060e-02,  3.3635e-02,  1.0393e-02, -1.0609e-01,  3.4536e-02,\n",
       "        -4.1543e-02, -3.3688e-02, -9.8293e-02, -3.1489e-02, -1.3386e-01,\n",
       "        -4.7277e-04,  1.2700e-02,  1.2781e-01, -3.0148e-02,  3.0323e-02,\n",
       "         1.7533e-02, -3.5349e-02,  5.9834e-02,  8.5422e-02, -2.9915e-02,\n",
       "        -2.3465e-02,  6.6311e-02,  1.2356e-02,  4.4686e-02, -1.9784e-03,\n",
       "         4.9175e-02, -1.8820e-02, -1.9994e-02,  1.2050e-02,  2.5195e-03,\n",
       "         2.8847e-02,  2.8978e-02, -5.3762e-02,  1.8725e-02,  3.4852e-03,\n",
       "         8.3855e-02, -4.8081e-02,  8.3227e-02, -8.4321e-02,  6.7735e-02,\n",
       "         1.8954e-03,  2.1204e-03, -1.0052e-02, -3.5380e-02, -2.8794e-02,\n",
       "        -7.0195e-02, -6.3278e-04, -5.9589e-02, -7.4508e-02,  2.3912e-02,\n",
       "         5.7519e-02,  6.2428e-02, -6.9434e-02,  9.2135e-03, -4.2549e-02,\n",
       "         4.8985e-04,  1.7289e-02,  6.0565e-02, -1.7265e-02,  6.5779e-03,\n",
       "        -3.6194e-02, -8.1181e-02, -4.9282e-03,  6.0724e-02, -2.4496e-02,\n",
       "        -3.5228e-02, -1.1260e-02, -7.1122e-02, -7.3829e-03, -7.3765e-02,\n",
       "        -2.4308e-02, -3.8375e-02, -2.1433e-02, -5.0508e-02, -5.5801e-02,\n",
       "        -6.8223e-02,  9.4096e-02,  4.5384e-02,  4.2533e-02,  2.9607e-02,\n",
       "         4.4982e-02, -5.6976e-02, -1.5208e-02, -1.2493e-01, -1.4041e-02,\n",
       "         5.4498e-02, -5.4206e-02,  8.0571e-02,  1.7485e-33, -2.7614e-02,\n",
       "        -8.1229e-02,  4.2715e-02,  5.7465e-03, -6.1910e-02, -8.0614e-02,\n",
       "        -9.7833e-03,  3.4079e-02,  7.4102e-02, -3.0596e-02, -1.5077e-02,\n",
       "         1.1601e-01,  9.7768e-03, -1.0370e-02,  3.2794e-03,  2.9906e-02,\n",
       "        -3.6497e-02,  3.7913e-02,  1.8539e-02,  9.1141e-03,  2.6190e-02,\n",
       "         1.0473e-01,  1.8052e-02,  7.9431e-02,  2.1429e-02,  6.4829e-02,\n",
       "         8.6203e-02, -2.5036e-02,  4.0616e-03, -1.5787e-01, -6.1516e-02,\n",
       "        -1.1945e-01, -4.6757e-02,  1.2255e-01,  2.3282e-02,  2.7503e-02,\n",
       "        -2.6040e-02,  1.0663e-01, -3.7811e-02,  2.8569e-02, -5.0794e-02,\n",
       "        -3.2301e-02,  2.7352e-02,  7.2442e-02,  2.9810e-02, -5.4999e-02,\n",
       "        -4.4316e-03, -1.5928e-02,  7.1472e-03,  3.9631e-02, -9.0324e-02,\n",
       "        -6.9832e-02,  6.7862e-02,  2.2656e-02, -8.9167e-02,  1.0106e-02,\n",
       "         2.7015e-02,  6.0552e-03, -4.4874e-02, -7.6820e-02,  8.5994e-03,\n",
       "         2.8116e-02, -4.6919e-02,  3.9526e-03,  5.5606e-02, -1.7569e-02,\n",
       "         2.0184e-02, -6.9658e-02, -1.3223e-02,  6.6950e-02, -2.5052e-02,\n",
       "         4.5661e-02,  8.3906e-03,  1.1578e-02,  1.8083e-02, -3.3492e-02,\n",
       "        -2.7878e-02,  6.4630e-02, -2.3808e-02, -9.4826e-03,  1.3005e-02,\n",
       "        -1.0051e-01, -6.0398e-05, -2.6352e-02, -7.4384e-02,  2.5440e-02,\n",
       "         1.0198e-01, -6.3461e-02, -1.0871e-02, -2.3095e-02, -2.3586e-02,\n",
       "         4.3761e-02,  3.3886e-03, -6.6261e-02, -8.5884e-02, -1.3806e-08,\n",
       "        -1.0366e-02,  2.4243e-02, -1.8488e-02,  3.2696e-02,  3.6833e-02,\n",
       "        -6.0909e-02, -3.9591e-02,  1.8942e-03, -4.2579e-02, -9.1713e-02,\n",
       "         2.5090e-02,  7.1150e-02,  5.2762e-02, -6.0810e-02,  2.8013e-02,\n",
       "        -3.0789e-02, -3.1649e-02, -1.9041e-02, -1.6860e-02,  1.2498e-02,\n",
       "        -9.9975e-02, -2.1981e-02,  1.9430e-02, -1.2670e-02, -4.5291e-02,\n",
       "         1.1832e-01,  5.4848e-02, -4.0015e-02,  9.8105e-02,  2.2277e-02,\n",
       "        -3.0813e-02, -5.1757e-03,  4.9103e-02,  4.5938e-02, -2.3188e-02,\n",
       "        -2.7573e-02, -4.0576e-02,  1.6116e-02,  2.5010e-02, -5.8007e-02,\n",
       "         4.7965e-02,  1.1796e-01, -8.9742e-03, -1.3361e-02,  2.0989e-02,\n",
       "        -2.5200e-02, -6.8961e-03, -2.1131e-02,  5.4617e-03,  6.4138e-02,\n",
       "         2.6008e-02, -2.9850e-02, -1.1776e-02,  3.0898e-03, -1.6169e-01,\n",
       "        -4.6426e-02,  6.0036e-03,  5.2813e-03, -3.3420e-03,  2.7754e-02,\n",
       "         2.0411e-02,  5.7778e-03,  3.4098e-02, -6.8891e-03])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_embed01 = model_embedding.encode(texts[0], convert_to_tensor=True)\n",
    "texts_embed01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafe1a0-026f-423f-8926-ec98fdd6f712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_embed02 = model_embedding.encode(texts[1], convert_to_tensor=True)\n",
    "texts_embed02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2157bd-f14b-43bd-a66e-d8b85aadf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7d335-d0f5-4862-8ff3-bf577eefa451",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_cos_sim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m similarity \u001b[39m=\u001b[39m pytorch_cos_sim(texts_embed01, texts_embed02)\n\u001b[1;32m      2\u001b[0m similarity\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch_cos_sim' is not defined"
     ]
    }
   ],
   "source": [
    "similarity = pytorch_cos_sim(texts_embed01, texts_embed02)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe16b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Work\n",
    "# try cosinesimilarity from math function\n",
    "# try a different dimension embedding 700+ and calulate cosine sim and perform other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecdf743-cbba-4256-a5b1-d839d3efb502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 384)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_texts = model_embedding.encode(texts)\n",
    "embedding_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66dbdff-e247-42fe-9531-3b66c68bc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "embed_df = DataFrame(embedding_texts)\n",
    "\n",
    "# modify the df to incorporate text in one colum and embeddings in other, try to use list , try to use dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fe9d4-969d-465b-b86c-8b9e71204262",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023889</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>-0.011655</td>\n",
       "      <td>-0.033414</td>\n",
       "      <td>-0.012261</td>\n",
       "      <td>-0.024873</td>\n",
       "      <td>-0.012663</td>\n",
       "      <td>0.025346</td>\n",
       "      <td>0.018509</td>\n",
       "      <td>-0.083508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161688</td>\n",
       "      <td>-0.046426</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>-0.003342</td>\n",
       "      <td>0.027754</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.006889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.012688</td>\n",
       "      <td>0.046874</td>\n",
       "      <td>-0.010502</td>\n",
       "      <td>-0.020384</td>\n",
       "      <td>-0.013361</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>-0.004099</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.010188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061594</td>\n",
       "      <td>-0.020717</td>\n",
       "      <td>-0.009082</td>\n",
       "      <td>-0.029260</td>\n",
       "      <td>-0.066253</td>\n",
       "      <td>0.065257</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>-0.023103</td>\n",
       "      <td>-0.002785</td>\n",
       "      <td>0.010474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>-0.051981</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.006111</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108326</td>\n",
       "      <td>-0.049646</td>\n",
       "      <td>-0.073399</td>\n",
       "      <td>-0.029898</td>\n",
       "      <td>-0.102734</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.034605</td>\n",
       "      <td>0.016877</td>\n",
       "      <td>-0.023861</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.029711</td>\n",
       "      <td>0.023298</td>\n",
       "      <td>-0.057041</td>\n",
       "      <td>-0.012183</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.029796</td>\n",
       "      <td>0.063739</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>-0.045124</td>\n",
       "      <td>-0.040747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117682</td>\n",
       "      <td>0.031924</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>-0.020666</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>-0.010255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025628</td>\n",
       "      <td>0.070388</td>\n",
       "      <td>-0.017380</td>\n",
       "      <td>-0.056567</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.052823</td>\n",
       "      <td>0.067063</td>\n",
       "      <td>-0.052618</td>\n",
       "      <td>-0.054702</td>\n",
       "      <td>-0.116230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118145</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>-0.055188</td>\n",
       "      <td>-0.032723</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.048212</td>\n",
       "      <td>-0.040412</td>\n",
       "      <td>0.083346</td>\n",
       "      <td>0.026855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.022656</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>-0.046494</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>0.054268</td>\n",
       "      <td>-0.024185</td>\n",
       "      <td>-0.013483</td>\n",
       "      <td>-0.075966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100110</td>\n",
       "      <td>0.010750</td>\n",
       "      <td>-0.031469</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.039657</td>\n",
       "      <td>0.026384</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.059089</td>\n",
       "      <td>-0.017509</td>\n",
       "      <td>0.007166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.002911</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>-0.006133</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.036594</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>-0.031345</td>\n",
       "      <td>0.031806</td>\n",
       "      <td>-0.023495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028763</td>\n",
       "      <td>-0.060458</td>\n",
       "      <td>-0.018598</td>\n",
       "      <td>-0.040189</td>\n",
       "      <td>-0.031486</td>\n",
       "      <td>-0.018299</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>-0.073420</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>-0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.080526</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>-0.048847</td>\n",
       "      <td>-0.040176</td>\n",
       "      <td>-0.063342</td>\n",
       "      <td>0.041848</td>\n",
       "      <td>0.119045</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>-0.030095</td>\n",
       "      <td>-0.004561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144566</td>\n",
       "      <td>0.020404</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>-0.055645</td>\n",
       "      <td>-0.007675</td>\n",
       "      <td>0.050791</td>\n",
       "      <td>-0.005989</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.034817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.034388</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>-0.036695</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.063070</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.059862</td>\n",
       "      <td>-0.045383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114763</td>\n",
       "      <td>-0.035894</td>\n",
       "      <td>-0.019877</td>\n",
       "      <td>-0.033375</td>\n",
       "      <td>-0.030168</td>\n",
       "      <td>0.039412</td>\n",
       "      <td>0.044993</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>-0.025124</td>\n",
       "      <td>0.034191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005964</td>\n",
       "      <td>0.025044</td>\n",
       "      <td>-0.003182</td>\n",
       "      <td>-0.025243</td>\n",
       "      <td>-0.039823</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>0.044713</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>-0.038213</td>\n",
       "      <td>-0.041149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057621</td>\n",
       "      <td>0.021594</td>\n",
       "      <td>0.048983</td>\n",
       "      <td>-0.044541</td>\n",
       "      <td>-0.030137</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.054854</td>\n",
       "      <td>0.029937</td>\n",
       "      <td>0.070214</td>\n",
       "      <td>0.041565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.039008</td>\n",
       "      <td>-0.010609</td>\n",
       "      <td>-0.007383</td>\n",
       "      <td>-0.050190</td>\n",
       "      <td>-0.002518</td>\n",
       "      <td>-0.041641</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>-0.014801</td>\n",
       "      <td>-0.014127</td>\n",
       "      <td>-0.061637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098168</td>\n",
       "      <td>-0.031693</td>\n",
       "      <td>-0.052127</td>\n",
       "      <td>0.014774</td>\n",
       "      <td>-0.091150</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.053866</td>\n",
       "      <td>-0.083904</td>\n",
       "      <td>0.037684</td>\n",
       "      <td>0.002314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.095983</td>\n",
       "      <td>-0.063012</td>\n",
       "      <td>-0.116906</td>\n",
       "      <td>-0.059075</td>\n",
       "      <td>-0.051323</td>\n",
       "      <td>-0.003439</td>\n",
       "      <td>0.018687</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>-0.049057</td>\n",
       "      <td>-0.031649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041085</td>\n",
       "      <td>-0.008593</td>\n",
       "      <td>-0.021544</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>-0.019502</td>\n",
       "      <td>0.050039</td>\n",
       "      <td>-0.029175</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.152892</td>\n",
       "      <td>0.024720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.011600</td>\n",
       "      <td>0.056510</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>-0.094690</td>\n",
       "      <td>-0.009865</td>\n",
       "      <td>0.072347</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>-0.041175</td>\n",
       "      <td>-0.042124</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135238</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>-0.049410</td>\n",
       "      <td>-0.006925</td>\n",
       "      <td>0.085355</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.030402</td>\n",
       "      <td>-0.029012</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>-0.000133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.023889  0.055259 -0.011655 -0.033414 -0.012261 -0.024873 -0.012663   \n",
       "1  -0.012688  0.046874 -0.010502 -0.020384 -0.013361  0.042322  0.016628   \n",
       "2   0.000494  0.119412  0.005229 -0.092734  0.007773 -0.005325  0.034506   \n",
       "3  -0.029711  0.023298 -0.057041 -0.012183 -0.013710  0.029796  0.063739   \n",
       "4  -0.025628  0.070388 -0.017380 -0.056567  0.028576  0.052823  0.067063   \n",
       "5  -0.022656  0.021160  0.005105 -0.046494  0.009074  0.041495  0.054268   \n",
       "6  -0.002911  0.060791 -0.009176 -0.006133  0.040492  0.036594  0.002054   \n",
       "7  -0.080526  0.059888 -0.048847 -0.040176 -0.063342  0.041848  0.119045   \n",
       "8  -0.034388  0.072501  0.014440 -0.036695  0.014019  0.063070  0.034683   \n",
       "9  -0.005964  0.025044 -0.003182 -0.025243 -0.039823 -0.012772  0.044713   \n",
       "10 -0.039008 -0.010609 -0.007383 -0.050190 -0.002518 -0.041641  0.026969   \n",
       "11 -0.095983 -0.063012 -0.116906 -0.059075 -0.051323 -0.003439  0.018687   \n",
       "12 -0.011600  0.056510  0.016624 -0.094690 -0.009865  0.072347  0.044124   \n",
       "\n",
       "         7         8         9    ...       374       375       376       377  \\\n",
       "0   0.025346  0.018509 -0.083508  ... -0.161688 -0.046426  0.006004  0.005281   \n",
       "1  -0.004099 -0.002607 -0.010188  ... -0.061594 -0.020717 -0.009082 -0.029260   \n",
       "2  -0.051981 -0.006265 -0.006111  ... -0.108326 -0.049646 -0.073399 -0.029898   \n",
       "3   0.001101 -0.045124 -0.040747  ... -0.117682  0.031924  0.000854  0.020200   \n",
       "4  -0.052618 -0.054702 -0.116230  ... -0.118145  0.013343 -0.055188 -0.032723   \n",
       "5  -0.024185 -0.013483 -0.075966  ... -0.100110  0.010750 -0.031469 -0.004822   \n",
       "6  -0.031345  0.031806 -0.023495  ... -0.028763 -0.060458 -0.018598 -0.040189   \n",
       "7   0.010652 -0.030095 -0.004561  ... -0.144566  0.020404  0.023088  0.005077   \n",
       "8  -0.014531 -0.059862 -0.045383  ... -0.114763 -0.035894 -0.019877 -0.033375   \n",
       "9   0.014535 -0.038213 -0.041149  ... -0.057621  0.021594  0.048983 -0.044541   \n",
       "10 -0.014801 -0.014127 -0.061637  ... -0.098168 -0.031693 -0.052127  0.014774   \n",
       "11  0.006544 -0.049057 -0.031649  ... -0.041085 -0.008593 -0.021544 -0.021112   \n",
       "12 -0.041175 -0.042124 -0.102631  ... -0.135238  0.013612 -0.049410 -0.006925   \n",
       "\n",
       "         378       379       380       381       382       383  \n",
       "0  -0.003342  0.027754  0.020411  0.005778  0.034098 -0.006889  \n",
       "1  -0.066253  0.065257  0.013229 -0.023103 -0.002785  0.010474  \n",
       "2  -0.102734  0.062121  0.034605  0.016877 -0.023861  0.005264  \n",
       "3  -0.020666 -0.005167  0.038370  0.003617  0.033993 -0.010255  \n",
       "4   0.008436  0.019169  0.048212 -0.040412  0.083346  0.026855  \n",
       "5   0.039657  0.026384  0.045514  0.059089 -0.017509  0.007166  \n",
       "6  -0.031486 -0.018299  0.002286 -0.073420  0.016235 -0.000244  \n",
       "7  -0.055645 -0.007675  0.050791 -0.005989  0.134562  0.034817  \n",
       "8  -0.030168  0.039412  0.044993  0.000578 -0.025124  0.034191  \n",
       "9  -0.030137  0.006779  0.054854  0.029937  0.070214  0.041565  \n",
       "10 -0.091150  0.001324  0.053866 -0.083904  0.037684  0.002314  \n",
       "11 -0.019502  0.050039 -0.029175  0.005498  0.152892  0.024720  \n",
       "12  0.085355 -0.007875  0.030402 -0.029012  0.001888 -0.000133  \n",
       "\n",
       "[13 rows x 384 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac270a3-ad77-482e-8f8a-0f6ca69ae21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df['texts'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddb6e7-704f-4f45-8965-a0f1cbd955f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df.to_csv(\"embed_text.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbcc3a-2130-4308-8db7-a5835209bd2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a5aed2bf534f0c92e7c816488950ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# pushing the dataset to huggingface hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# pj2111/embedding_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b5998-d12f-452d-9c37-4f71d607fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efea99-30fc-44d8-8966-51ec2e6b19b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/datasets/table.py:720: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  return cls(pa.Table.from_pandas(*args, **kwargs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', 'texts'],\n",
       "    num_rows: 13\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ds = Dataset.from_pandas(embed_df)\n",
    "embed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238060ab-8e80-4b8d-88d0-3039bce778de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ds = embed_ds.train_test_split(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82a224-7b6f-4351-95f1-082a88b24807",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', 'texts'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', 'texts'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73fa5b-be79-4d9d-96a7-3977f6667977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707df0fdf0044da0b133a59e738d3fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af07180326bd480eb34585a7b7d64ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03e643046f04817ad9b9209601f11de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de34fdeac63495192e84c89c945cc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/pj2111/embedding_tutorial/commit/00a801a13b56e55a1856943d6c74aec95986e179', commit_message='Upload dataset', commit_description='', oid='00a801a13b56e55a1856943d6c74aec95986e179', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_path = \"Kamaljp/embed_texts\"\n",
    "dataset_path = \"pj2111/embedding_tutorial\"\n",
    "embed_ds.push_to_hub(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c508eeb-dbb7-4434-a338-3bdfc84e8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some models with mandatory prompts,\n",
    "\n",
    "allmini = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "prompts = {\n",
    "    \"classification\": \"Classify the following text:\",\n",
    "    \"retrieval\": \"Retrieve Semantically Similar text:\",\n",
    "    \"Clustering\": \"Identify the topic or theme based on the text:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cc5a2-9b1d-4711-bf72-682c4077ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiling_model = SentenceTransformer(\n",
    "    model_name_or_path=allmini,\n",
    "    prompts=prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c777d-25c0-44f1-9fe4-4be9af845e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# during embedding, the following process is followed\n",
    "\n",
    "embeddings = multiling_model.encode(\"There are many good looking places\",\n",
    "                                    prompt_name='retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170917c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa569b8b-a111-47e0-8be6-d876880a7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the Transformers AutoClasses, will require additional steps to access the embedding\n",
    "# We cannot dismantle the model so we need to dissect the model output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cf21f-3e50-4de9-af3c-c5b1bdca0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(allmini)\n",
    "model = AutoModel.from_pretrained(allmini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ba99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers/all-MiniLM-L6-v2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30522</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sep_token'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cls_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mask_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">added_tokens_decoder</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddedToken</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[PAD]\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">rstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">single_word</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">normalized</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddedToken</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[UNK]\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">rstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">single_word</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">normalized</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddedToken</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[CLS]\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">rstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">single_word</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">normalized</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddedToken</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[SEP]\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">rstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">single_word</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">normalized</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">103</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddedToken</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[MASK]\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">rstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">single_word</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">normalized</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mBertTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'sentence-transformers/all-MiniLM-L6-v2'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m30522\u001b[0m, \u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m512\u001b[0m, \n",
       "\u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUNK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'sep_token'\u001b[0m: \n",
       "\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mPAD\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'cls_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'mask_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,  \n",
       "\u001b[33madded_tokens_decoder\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[1;36m0\u001b[0m: \u001b[1;35mAddedToken\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mPAD\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m100\u001b[0m: \u001b[1;35mAddedToken\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUNK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m101\u001b[0m: \u001b[1;35mAddedToken\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m102\u001b[0m: \u001b[1;35mAddedToken\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m103\u001b[0m: \u001b[1;35mAddedToken\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertModel</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertEmbeddings</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>word_embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30522</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_idx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>position_embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>token_type_embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>LayerNorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-12</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>encoder<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertEncoder</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>layer<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> x <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertLayer</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"font-weight: bold\">(</span>attention<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertAttention</span><span style=\"font-weight: bold\">(</span>\n",
       "          <span style=\"font-weight: bold\">(</span>self<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertSelfAttention</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">(</span>query<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">(</span>key<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">(</span>value<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertSelfOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">(</span>LayerNorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-12</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>intermediate<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertIntermediate</span><span style=\"font-weight: bold\">(</span>\n",
       "          <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>intermediate_act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GELUActivation</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "          <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>LayerNorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-12</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "          <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>pooler<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BertPooler</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span>activation<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Tanh</span><span style=\"font-weight: bold\">()</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mBertModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0membeddings\u001b[1m)\u001b[0m: \u001b[1;35mBertEmbeddings\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mword_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m30522\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mpadding_idx\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m384\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mtoken_type_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m384\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mLayerNorm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-12\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mencoder\u001b[1m)\u001b[0m: \u001b[1;35mBertEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayer\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m6\u001b[0m x \u001b[1;35mBertLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattention\u001b[1m)\u001b[0m: \u001b[1;35mBertAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mself\u001b[1m)\u001b[0m: \u001b[1;35mBertSelfAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[1m(\u001b[0mquery\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mkey\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mvalue\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;35mBertSelfOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mLayerNorm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-12\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mintermediate\u001b[1m)\u001b[0m: \u001b[1;35mBertIntermediate\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mintermediate_act_fn\u001b[1m)\u001b[0m: \u001b[1;35mGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;35mBertOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mLayerNorm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-12\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpooler\u001b[1m)\u001b[0m: \u001b[1;35mBertPooler\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mactivation\u001b[1m)\u001b[0m: \u001b[1;35mTanh\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf57e21-db4b-4df0-9675-710d27f279ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This framework generate embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of strings\",\n",
    "    \"The quick brown fox jupms over the lazy dog.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8d5a6-3977-4907-a072-cca73af31f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_sentence = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247ace0-3e95-41a6-8dcd-4a54468b2fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd9e2c-e061-4724-abae-89bf1d036c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When getting the inference, the encoded tokens are sent into the models for processing\n",
    "# first step in the model is embedding\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():  # We don't want the model to calculate the gradient, when making this pass\n",
    "    model_out = model(**encode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db671b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BaseModelOutputWithPoolingAndCrossAttentions</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last_hidden_state</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2893</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2586</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1951</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4487</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0156</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2387</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6280</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0351</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2810</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5791</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1623</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6111</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0042</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2457</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3973</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2251</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3979</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4204</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0323</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2735</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6323</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9919</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9204</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0780</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2916</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0552</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0531</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0686</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1934</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1630</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1305</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3905</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5393</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2618</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1214</span><span style=\"font-weight: bold\">]]</span>,\n",
       "\n",
       "        <span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0927</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2340</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0215</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1279</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1010</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1594</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1660</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0497</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6604</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7315</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1036</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2432</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0886</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4472</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3356</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2749</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4591</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3030</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3019</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3648</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7661</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5218</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6613</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3835</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3017</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2839</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7455</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5049</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5942</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3760</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2619</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2673</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7375</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5059</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5387</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3918</span><span style=\"font-weight: bold\">]]</span>,\n",
       "\n",
       "        <span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1232</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4818</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2373</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0889</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2035</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1028</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1778</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2701</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2238</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1792</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3968</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1828</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3411</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4414</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5761</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1566</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5203</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5159</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3415</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3199</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2400</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4144</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9339</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0002</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0504</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3801</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1554</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0059</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0598</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5151</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1483</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1651</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4039</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1983</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2872</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2963</span><span style=\"font-weight: bold\">]]])</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">pooler_output</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0418</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0009</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0381</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0102</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0633</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0043</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0250</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0342</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0144</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1159</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0223</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0089</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0468</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0927</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0213</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0409</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0306</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">hidden_states</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">past_key_values</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">attentions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cross_attentions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mBaseModelOutputWithPoolingAndCrossAttentions\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mlast_hidden_state\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2893\u001b[0m, \u001b[1;36m-0.2586\u001b[0m, \u001b[1;36m-0.1951\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.4487\u001b[0m,  \u001b[1;36m0.0156\u001b[0m, \u001b[1;36m-0.2387\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.6280\u001b[0m, \u001b[1;36m-0.0351\u001b[0m, \u001b[1;36m-0.2810\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.5791\u001b[0m,  \u001b[1;36m1.1623\u001b[0m,  \u001b[1;36m0.6111\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.0042\u001b[0m, \u001b[1;36m-0.2457\u001b[0m, \u001b[1;36m-0.3973\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-0.2251\u001b[0m,  \u001b[1;36m1.3979\u001b[0m, \u001b[1;36m-0.4204\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[33m...\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.0323\u001b[0m, \u001b[1;36m-0.2735\u001b[0m,  \u001b[1;36m0.6323\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.9919\u001b[0m,  \u001b[1;36m0.9204\u001b[0m, \u001b[1;36m-1.0780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.2916\u001b[0m,  \u001b[1;36m0.0552\u001b[0m,  \u001b[1;36m0.0531\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m1.0686\u001b[0m,  \u001b[1;36m0.1934\u001b[0m, \u001b[1;36m-0.1630\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.7100\u001b[0m, \u001b[1;36m-0.1305\u001b[0m,  \u001b[1;36m0.3905\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.5393\u001b[0m,  \u001b[1;36m0.2618\u001b[0m, \u001b[1;36m-0.1214\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0927\u001b[0m,  \u001b[1;36m0.2340\u001b[0m,  \u001b[1;36m0.0215\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.1279\u001b[0m, \u001b[1;36m-0.1010\u001b[0m, \u001b[1;36m-0.1594\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.1660\u001b[0m,  \u001b[1;36m0.0497\u001b[0m,  \u001b[1;36m0.6604\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.7315\u001b[0m,  \u001b[1;36m0.1036\u001b[0m, \u001b[1;36m-1.2432\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.0886\u001b[0m,  \u001b[1;36m0.4472\u001b[0m,  \u001b[1;36m0.3356\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.2749\u001b[0m,  \u001b[1;36m0.4591\u001b[0m, \u001b[1;36m-0.3030\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[33m...\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.3019\u001b[0m,  \u001b[1;36m0.3648\u001b[0m,  \u001b[1;36m0.7661\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.5218\u001b[0m,  \u001b[1;36m0.6613\u001b[0m, \u001b[1;36m-0.3835\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.3017\u001b[0m,  \u001b[1;36m0.2839\u001b[0m,  \u001b[1;36m0.7455\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.5049\u001b[0m,  \u001b[1;36m0.5942\u001b[0m, \u001b[1;36m-0.3760\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.2619\u001b[0m,  \u001b[1;36m0.2673\u001b[0m,  \u001b[1;36m0.7375\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.5059\u001b[0m,  \u001b[1;36m0.5387\u001b[0m, \u001b[1;36m-0.3918\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.1232\u001b[0m,  \u001b[1;36m0.4818\u001b[0m,  \u001b[1;36m0.2373\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.0889\u001b[0m, \u001b[1;36m-0.2035\u001b[0m, \u001b[1;36m-0.1028\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-0.1778\u001b[0m,  \u001b[1;36m0.2701\u001b[0m,  \u001b[1;36m0.2238\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-0.1792\u001b[0m,  \u001b[1;36m0.3968\u001b[0m,  \u001b[1;36m1.1828\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.3411\u001b[0m,  \u001b[1;36m0.4414\u001b[0m,  \u001b[1;36m0.5761\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m1.1566\u001b[0m,  \u001b[1;36m0.5203\u001b[0m,  \u001b[1;36m0.5159\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[33m...\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.3415\u001b[0m,  \u001b[1;36m0.3199\u001b[0m,  \u001b[1;36m0.2400\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.4144\u001b[0m,  \u001b[1;36m0.9339\u001b[0m,  \u001b[1;36m1.0002\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.0504\u001b[0m,  \u001b[1;36m0.3801\u001b[0m,  \u001b[1;36m0.1554\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-0.0059\u001b[0m, \u001b[1;36m-0.0598\u001b[0m,  \u001b[1;36m0.5151\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.1483\u001b[0m,  \u001b[1;36m0.1651\u001b[0m,  \u001b[1;36m0.4039\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-0.1983\u001b[0m,  \u001b[1;36m0.2872\u001b[0m,  \u001b[1;36m0.2963\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mpooler_output\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0418\u001b[0m,  \u001b[1;36m0.0009\u001b[0m,  \u001b[1;36m0.0381\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.0102\u001b[0m, \u001b[1;36m-0.0633\u001b[0m, \u001b[1;36m-0.0043\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.0250\u001b[0m, \u001b[1;36m-0.0342\u001b[0m, \u001b[1;36m-0.0144\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.0388\u001b[0m, \u001b[1;36m-0.1159\u001b[0m,  \u001b[1;36m0.0223\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.0089\u001b[0m,  \u001b[1;36m0.0468\u001b[0m,  \u001b[1;36m0.0927\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.0213\u001b[0m, \u001b[1;36m-0.0409\u001b[0m, \u001b[1;36m-0.0306\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mhidden_states\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mpast_key_values\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mattentions\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mcross_attentions\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2350af-ed46-4ce0-a35e-04068a10d777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_out)  # observe the type of the model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b8942-975a-4240-a1de-d64386916425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 384])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds = model_out[0]\n",
    "token_embeds.shape  # Try to explain the shape, by relooking at the earlier steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdb0aa-3a93-414c-93d1-a5d2d75b7524",
   "metadata": {},
   "source": [
    "#### Next mean-pooling is not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a52cd-ebf5-49bf-ac3f-9d23ab2c77ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets seperate the attn_mask\n",
    "attn_mask = encode_sentence[\"attention_mask\"]\n",
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b1f0a-80ff-4b29-92e9-24ab7440652f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the process of Mean Pooling manually. This step is done for model inference\n",
    "# expanding the attn_masks\n",
    "attn_mask_expanded = attn_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "# attn_mask_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93bed8-18cf-43ea-ba74-ed6d739502b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_embeds = torch.sum(token_embeds * attn_mask_expanded , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2aace-c775-43cc-9e5e-68dfb40a05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mask = torch.clamp(attn_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b4164-7d52-4eaa-a1ca-f384d0b8f4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0923, -0.2241, -0.0747,  ...,  0.5802,  0.6137, -0.2178],\n",
       "        [ 0.2897,  0.2838,  0.2554,  ...,  0.2671,  0.3355, -0.1703],\n",
       "        [ 0.1504,  0.4607,  0.2637,  ...,  0.0908,  0.3025,  0.3073]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = sum_embeds / sum_mask\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b4b45-2b79-4349-8607-ce7ee083b0da",
   "metadata": {},
   "source": [
    "#### Some Tasks with Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915a3eb-cff6-4fa5-9816-4ac83c947b6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8315f-409b-440a-8a5d-a2a9645ddd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Similarity\n",
    "\n",
    "sentences2 = [\n",
    "    # 'There is more to embeddings than it meets the eyes',\n",
    "    'A man is eating food.',\n",
    "    'Every object in the Neural Network world have a rich and varied back story',\n",
    "    'when there are instances, that means there has to be blueprints of them lying around'\n",
    "]\n",
    "sentences = ['This framework generate embeddings for each input sentence',\n",
    "             'A man is eating pasta.',\n",
    "#  'Sentences are passed as a list of strings',\n",
    " 'The quick brown fox jupms over the lazy dog.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b4201-c034-44e3-8029-f701be43bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model_embedding.encode(sentences)\n",
    "embed2 = model_embedding.encode(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfb765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0312,  0.7035, -0.0216],\n",
       "        [ 0.3569,  0.0551,  0.1245],\n",
       "        [ 0.1035, -0.1152,  0.1032]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the cosine scores\n",
    "cos_scores = util.cos_sim(embed2, embed)\n",
    "cos_scores  # the scores for each sentence in one embeding list is compared with another embeding list\n",
    "# So there will be a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cadcb-f267-4bbb-949b-b9500dda6802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3594, 0.1727, 0.1343],\n",
       "        [0.3569, 0.1764, 0.1245],\n",
       "        [0.1035, 0.1637, 0.1032]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the cosine scores\n",
    "cos_scores = util.cos_sim(embed2, embed)\n",
    "cos_scores  # the scores for each sentence in one embeding list is compared with another embeding list\n",
    "# So there will be a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81627534-564a-4cd7-93b8-2474d37d3432",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### semantic search\n",
    "\n",
    "Symmetric search: Query and the retrieved sentences are having the same length\n",
    "[link](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models)\n",
    "\n",
    "Assymetric search: Query is smaller in size, while the sentences are longer, like paras\n",
    "[link](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0ff5f-12f3-43ba-a23f-34afa805d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"A cheetah is running behind its prey.\",\n",
    "]\n",
    "\n",
    "# Query sentences:\n",
    "queries = [\n",
    "    \"A man is eating pasta.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
    "    \"A cheetah chases prey on across a field.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb919d-9fe9-46d5-92f8-2576f77e6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import (\n",
    "    normalize_embeddings,\n",
    "    dot_score,\n",
    "    semantic_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b222148-f75b-49e7-b406-adc669f4dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model_embedding.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239a995-955d-472d-ba09-9f6d3bdbad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings =  corpus_embeddings.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758a283-a85a-43b0-94f9-cf5b1b257299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 384])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f587a-4498-4111-8744-348ee8626d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0332,  0.0044, -0.0063,  ...,  0.0692, -0.0246, -0.0376],\n",
       "        [ 0.0525,  0.0552, -0.0112,  ..., -0.0162, -0.0602, -0.0412],\n",
       "        [-0.0363, -0.0357, -0.0272,  ..., -0.0386,  0.1057, -0.0013],\n",
       "        ...,\n",
       "        [ 0.0370,  0.0226,  0.0496,  ..., -0.0031,  0.0489,  0.0167],\n",
       "        [ 0.0235, -0.0585,  0.0560,  ...,  0.0584,  0.0377,  0.0410],\n",
       "        [ 0.0228,  0.1041, -0.0340,  ...,  0.0029,  0.0386,  0.0438]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized corpus embeddings make it simple to calculat the dot-pdt\n",
    "corpus_embeddings = normalize_embeddings(corpus_embeddings)\n",
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f002c-0d7f-41cb-858a-4d71ebe21f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0116, -0.0508, -0.0217,  ...,  0.0822,  0.0099, -0.0394],\n",
       "        [-0.0357,  0.0168,  0.0448,  ...,  0.0249,  0.0653, -0.0112],\n",
       "        [ 0.0544,  0.0540, -0.0037,  ...,  0.0325,  0.0219,  0.0621]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = model_embedding.encode(queries, convert_to_tensor=True)\n",
    "query_embeddings = normalize_embeddings(query_embeddings.to('cuda'))\n",
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9acd7-bc8c-4db0-b66d-5f071f8040c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search = semantic_search(query_embeddings, corpus_embeddings, score_function=dot_score)\n",
    "# create a threshold score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f579a4b-68ee-441a-a5f3-d5bd7fa0766a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 0, 'score': 0.7035486698150635},\n",
       "  {'corpus_id': 1, 'score': 0.5271987318992615},\n",
       "  {'corpus_id': 3, 'score': 0.18889553844928741},\n",
       "  {'corpus_id': 6, 'score': 0.10469923168420792},\n",
       "  {'corpus_id': 8, 'score': 0.09803037345409393},\n",
       "  {'corpus_id': 7, 'score': 0.08189043402671814},\n",
       "  {'corpus_id': 4, 'score': 0.033593956381082535},\n",
       "  {'corpus_id': 5, 'score': -0.059434838593006134},\n",
       "  {'corpus_id': 2, 'score': -0.08980069309473038}],\n",
       " [{'corpus_id': 7, 'score': 0.6432533264160156},\n",
       "  {'corpus_id': 4, 'score': 0.25641557574272156},\n",
       "  {'corpus_id': 3, 'score': 0.1388726532459259},\n",
       "  {'corpus_id': 6, 'score': 0.11909151822328568},\n",
       "  {'corpus_id': 8, 'score': 0.10798682272434235},\n",
       "  {'corpus_id': 0, 'score': 0.06300687044858932},\n",
       "  {'corpus_id': 2, 'score': 0.02465788647532463},\n",
       "  {'corpus_id': 1, 'score': 0.021566985175013542},\n",
       "  {'corpus_id': 5, 'score': -0.08950325846672058}],\n",
       " [{'corpus_id': 8, 'score': 0.8253214359283447},\n",
       "  {'corpus_id': 0, 'score': 0.1398952305316925},\n",
       "  {'corpus_id': 7, 'score': 0.12919363379478455},\n",
       "  {'corpus_id': 6, 'score': 0.10974162817001343},\n",
       "  {'corpus_id': 3, 'score': 0.06497804075479507},\n",
       "  {'corpus_id': 1, 'score': 0.034424006938934326},\n",
       "  {'corpus_id': 2, 'score': 0.015606727451086044},\n",
       "  {'corpus_id': 5, 'score': -0.015542411245405674},\n",
       "  {'corpus_id': 4, 'score': -0.033420782536268234}]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69710b-5843-452c-b2d1-bf81ad26551d",
   "metadata": {},
   "source": [
    "##### Many real world problems can be solved using the above embedding models\n",
    "\n",
    "- Using ANN to search for getting the context for the RAG\n",
    "\n",
    "- Retrieve Similar questions, or similar problems, similar products based on something chosen by the user\n",
    "\n",
    "- Ranking the retrieved snippets for relevancy is another interesting task that can enrich the search process-\n",
    "\n",
    "- Clustering the sentences into topics or ideas\n",
    "\n",
    "- Paraphrase mining of large corpus texts with similar meaning or idea\n",
    "\n",
    "- Image Search using the embedded data of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cceeeff-2f02-4f53-b609-2448934c5a55",
   "metadata": {},
   "source": [
    "##### Retrieve & Re-Rank\n",
    "\n",
    "We will be using the CrossEncoder modlel cross-encoder/ms-marco-MiniLM-L-6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a420c2-ba66-44c8-9dd0-2c79b694a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages (from rank_bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbb9f3-cb69-43dc-b6a4-cc580ec5a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6aa74-f8cf-4bb7-8678-f83759d6347e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5697239e38a4dc7a959c6c708c6d0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba23d8a662849bca764500867b19a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad3a388057e4b2fa8cd63513e66203a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eb7199792d49aa9dbe27db53ce2ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946824ecfd5a4f5f976d6555bc7259d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb74534b852142dfa029fb79de1b3264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20e1f0c221b4e61857af4ff48893204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55928c660c445cfadc4db023d9a5c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4b5429bf4e4f4ba9f9818253901100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2180fc3bfdb41f898570af9701ab0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc4455d7fd3404c9899da077fe5c1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_qa = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "bi_encoder = SentenceTransformer(multi_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9a496-78e3-49f2-ab32-ee837696689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fb113093614b53a4c41a55c6ba82ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aacb68e908c4a97a192fb9155ee6442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e94d11fb314c5ea1f403a79719f01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a69ecdb7c94ff0bb325836638e140b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd7cc19a2fa4daaa889cf7c32b8c58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_enc = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "cross_encoder = CrossEncoder(cross_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acaf745-aaf6-4b54-967f-9b0bfb745976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cde253ae5ec42c3acda20e52cefc180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/50.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052f6b7-8cec-4e93-96ea-14bbcff894f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 169597\n"
     ]
    }
   ],
   "source": [
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '798870',\n",
       " 'title': 'Seminole bat',\n",
       " 'paragraphs': ['The Seminole bat (\"Lasiurus seminolus\") is a type of bat in the family Vespertilionidae.',\n",
       "  'The Seminole bat is often confused with the red bat. The Seminole bat has a mahogany color with a frosted look because to white tipped dorsal hairs. They weigh around 12 grams. Females are larger than males.',\n",
       "  'The Seminole bat is found in the Southeastern United States. This includes Louisiana, Georgia, Alabama, Mississippi, South Carolina and parts of Texas, Tennessee, Arkansas and North Carolina. It has also been seen as far as Mexico. It is a migratory species. In the winter, it lives along the Gulf Coast, North and South Carolina, and southern Arkansas. In the summer, they migrate as far north as Missouri and Kentucky.',\n",
       "  'It prefers to live in forested areas. In winter, they are found to use leaf litter and Spanish moss as insulation in their roost sites.',\n",
       "  'Seminole bats are insectivores. They eat large amounts of Hymenoptera (ants, bees and wasps), Coleoptera (beetles), Lepidoptera (moths). They have also been shown to eat smaller amounts of Homoptera (cicadas) and Diptera (flies).']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2438661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".',\n",
       " 'Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.',\n",
       " \"A crater is a round dent on a planet. They are usually shaped like a circle or an oval. They are usually made by something like a meteor hitting the surface of a planet. Underground activity such as volcanoes or explosions can also cause them but it's not as likely.\",\n",
       " 'Store has several meanings:',\n",
       " 'Chinese New Year, known in China as the SpringFestival and in Singapore as the LunarNewYear, is a holiday on and around the new moon on the first day of the year in the traditional Chinese calendar. This calendar is based on the changes in the moon and is only sometimes changed to fit the seasons of the year based on how the Earth moves around the sun. Because of this, Chinese New Year is never on January1. It moves around between January21 and February20.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554203a7-3506-4a2c-a015-09e2909e1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder = bi_encoder.to(\"cuda\")  # In CPU will take long time, items/sec rate will be 10/ 15. With "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8d750-0ccf-4b58-b6c6-3a318a9e13c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dce2eb074c4dae8e20f2f33a0e45ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embed the entire 170K passages with bi-encoder\n",
    "corpus_embeding = bi_encoder.encode(passages[:10], convert_to_tensor=True, show_progress_bar=True)#, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e7a4e-f8b0-45dd-bbc0-c328d0887c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30604b-22c3-4980-96c4-bfb56003688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddec38-9498-4984-b5d2-b20ab5faa4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4f927237cd4677be9437985aef785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages[:10]):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ted',\n",
       "  'cassidy',\n",
       "  'july',\n",
       "  '31',\n",
       "  '1932',\n",
       "  'january',\n",
       "  '16',\n",
       "  '1979',\n",
       "  'american',\n",
       "  'actor',\n",
       "  'best',\n",
       "  'known',\n",
       "  'roles',\n",
       "  'lurch',\n",
       "  'thing',\n",
       "  'addams',\n",
       "  'family'],\n",
       " ['aileen',\n",
       "  'carol',\n",
       "  'wuornos',\n",
       "  'pralle',\n",
       "  'born',\n",
       "  'aileen',\n",
       "  'carol',\n",
       "  'pittman',\n",
       "  'february',\n",
       "  '29',\n",
       "  '1956',\n",
       "  '–',\n",
       "  'october',\n",
       "  '9',\n",
       "  '2002',\n",
       "  'american',\n",
       "  'serial',\n",
       "  'killer',\n",
       "  'born',\n",
       "  'rochester',\n",
       "  'michigan',\n",
       "  'confessed',\n",
       "  'killing',\n",
       "  'men',\n",
       "  'florida',\n",
       "  'executed',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'prison',\n",
       "  'lethal',\n",
       "  'injection',\n",
       "  'murders',\n",
       "  'wuornos',\n",
       "  'said',\n",
       "  'men',\n",
       "  'killed',\n",
       "  'raped',\n",
       "  'tried',\n",
       "  'rape',\n",
       "  'working',\n",
       "  'prostitute'],\n",
       " ['crater',\n",
       "  'round',\n",
       "  'dent',\n",
       "  'planet',\n",
       "  'usually',\n",
       "  'shaped',\n",
       "  'like',\n",
       "  'circle',\n",
       "  'oval',\n",
       "  'usually',\n",
       "  'like',\n",
       "  'meteor',\n",
       "  'hitting',\n",
       "  'surface',\n",
       "  'planet',\n",
       "  'underground',\n",
       "  'activity',\n",
       "  'volcanoes',\n",
       "  'explosions',\n",
       "  'cause',\n",
       "  \"it's\",\n",
       "  'likely'],\n",
       " ['store', 'meanings'],\n",
       " ['chinese',\n",
       "  'new',\n",
       "  'year',\n",
       "  'known',\n",
       "  'china',\n",
       "  'springfestival',\n",
       "  'singapore',\n",
       "  'lunarnewyear',\n",
       "  'holiday',\n",
       "  'new',\n",
       "  'moon',\n",
       "  'day',\n",
       "  'year',\n",
       "  'traditional',\n",
       "  'chinese',\n",
       "  'calendar',\n",
       "  'calendar',\n",
       "  'based',\n",
       "  'changes',\n",
       "  'moon',\n",
       "  'changed',\n",
       "  'fit',\n",
       "  'seasons',\n",
       "  'year',\n",
       "  'based',\n",
       "  'earth',\n",
       "  'moves',\n",
       "  'sun',\n",
       "  'chinese',\n",
       "  'new',\n",
       "  'year',\n",
       "  'january1',\n",
       "  'moves',\n",
       "  'january21',\n",
       "  'february20'],\n",
       " ['music',\n",
       "  'suite',\n",
       "  'pronounce',\n",
       "  'sweet',\n",
       "  'collection',\n",
       "  'short',\n",
       "  'musical',\n",
       "  'pieces',\n",
       "  'played',\n",
       "  'pieces',\n",
       "  'usually',\n",
       "  'dance',\n",
       "  'movements',\n",
       "  'french',\n",
       "  'word',\n",
       "  '“suite”',\n",
       "  'means',\n",
       "  '“a',\n",
       "  'sequence”',\n",
       "  'things',\n",
       "  'i.e',\n",
       "  'thing',\n",
       "  'following'],\n",
       " ['michael',\n",
       "  'te-pei',\n",
       "  'chang',\n",
       "  'born',\n",
       "  'february',\n",
       "  '22',\n",
       "  '1972',\n",
       "  'american',\n",
       "  'retired',\n",
       "  'tennis',\n",
       "  'player',\n",
       "  'youngest-ever',\n",
       "  'male',\n",
       "  'winner',\n",
       "  'grand',\n",
       "  'slam',\n",
       "  'singles',\n",
       "  'title',\n",
       "  'michael',\n",
       "  'chang',\n",
       "  'did',\n",
       "  'won',\n",
       "  'french',\n",
       "  'open',\n",
       "  '1989',\n",
       "  'age',\n",
       "  '17'],\n",
       " ['catalão',\n",
       "  'brazilian',\n",
       "  'city',\n",
       "  'state',\n",
       "  'goiás',\n",
       "  '75.623',\n",
       "  'inhabitants',\n",
       "  'covers',\n",
       "  'founded',\n",
       "  '1833',\n",
       "  'today',\n",
       "  'important',\n",
       "  'industrial',\n",
       "  'center',\n",
       "  'state',\n",
       "  'goiás'],\n",
       " ['equus',\n",
       "  'genus',\n",
       "  'mammals',\n",
       "  'family',\n",
       "  'equidae',\n",
       "  'includes',\n",
       "  'horses',\n",
       "  'asses',\n",
       "  'zebras',\n",
       "  'equus',\n",
       "  'living',\n",
       "  'extant',\n",
       "  'genus',\n",
       "  'horses',\n",
       "  'seven',\n",
       "  'living',\n",
       "  'species',\n",
       "  'one-toed',\n",
       "  'horses',\n",
       "  'adapted',\n",
       "  'living',\n",
       "  'various',\n",
       "  'types',\n",
       "  'grasslands'],\n",
       " ['216',\n",
       "  'kleopatra',\n",
       "  'main',\n",
       "  'belt',\n",
       "  'asteroid',\n",
       "  'johann',\n",
       "  'palisa',\n",
       "  'april',\n",
       "  '10',\n",
       "  '1880',\n",
       "  'pola',\n",
       "  'named',\n",
       "  'cleopatra',\n",
       "  'queen',\n",
       "  'egypt']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8a909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rank_bm25.BM25Okapi at 0x7ff188790b20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38d72a-86e9-4b77-b316-a4cde10e0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=32)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking ##### Try to explain this part \n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740751bd-6236-47ef-8b83-b67e38a52294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Who is the president of India\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t14.509\tThe Vice President of India is the second-highest constitutional official in India, after the President.\n",
      "\t13.316\tFakhruddin Ali Ahmed was the fifth President of India from 1974 to 1977 and also the 2nd President of India to die in office.\n",
      "\t11.866\tThe President of India is the head of state of the Republic of India. The current president, Ram Nath Kovind, who was sworn in on 25 July 2017. He succeeded Pranab Mukherjee. The President resides in an estate known as the Rashtrapati Bhavan in New Delhi.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.119\tTed Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".\n",
      "\t0.077\tAileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956 – October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.\n",
      "\t0.065\tA crater is a round dent on a planet. They are usually shaped like a circle or an oval. They are usually made by something like a meteor hitting the surface of a planet. Underground activity such as volcanoes or explosions can also cause them but it's not as likely.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t-10.718\tMichael Te-Pei Chang (; born February 22, 1972) is an American retired tennis player. He was the youngest-ever male winner of a Grand Slam singles title. Michael Chang did this when he won the French Open in 1989 at the age of 17.\n",
      "\t-10.718\tAileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956 – October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.\n",
      "\t-10.887\tTed Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".\n"
     ]
    }
   ],
   "source": [
    "search(query=\"Who is the president of India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf983fa7-0926-4172-b93b-45134860fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
      "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6788\n",
      "I love pasta \t\t Do you like pizza? \t\t Score: 0.5096\n",
      "I love pasta \t\t The new movie is so great \t\t Score: 0.2560\n",
      "I love pasta \t\t The new movie is awesome \t\t Score: 0.2440\n",
      "A man is playing guitar \t\t The cat plays in the garden \t\t Score: 0.2105\n",
      "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1969\n",
      "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1692\n",
      "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1310\n",
      "The cat plays in the garden \t\t Do you like pizza? \t\t Score: 0.0900\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Single list of sentences - Possible tens of thousands of sentences\n",
    "sentences = [\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "    \"I love pasta\",\n",
    "    \"The new movie is awesome\",\n",
    "    \"The cat plays in the garden\",\n",
    "    \"A woman watches TV\",\n",
    "    \"The new movie is so great\",\n",
    "    \"Do you like pizza?\",\n",
    "]\n",
    "\n",
    "paraphrases = util.paraphrase_mining(model, sentences)\n",
    "\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bac1ad-87d1-4652-91d5-cc631d58d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb804b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task sentence-similarity, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msentence-similarity\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:859\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         pipeline_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    853\u001b[0m             class_ref,\n\u001b[1;32m    854\u001b[0m             model,\n\u001b[1;32m    855\u001b[0m             code_revision\u001b[39m=\u001b[39mcode_revision,\n\u001b[1;32m    856\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[1;32m    857\u001b[0m         )\n\u001b[1;32m    858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 859\u001b[0m     normalized_task, targeted_task, task_options \u001b[39m=\u001b[39m check_task(task)\n\u001b[1;32m    860\u001b[0m     \u001b[39mif\u001b[39;00m pipeline_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m         pipeline_class \u001b[39m=\u001b[39m targeted_task[\u001b[39m\"\u001b[39m\u001b[39mimpl\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:543\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_task\u001b[39m(task: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, Dict, Any]:\n\u001b[1;32m    499\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mreturn\u001b[39;00m PIPELINE_REGISTRY\u001b[39m.\u001b[39;49mcheck_task(task)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1281\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[39mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[39m1\u001b[39m], tokens[\u001b[39m3\u001b[39m])\n\u001b[1;32m   1279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid translation task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m\u001b[39m format\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1281\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m   1282\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, available tasks are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_supported_tasks()\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1283\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task sentence-similarity, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "# using sentence_transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"sentence-similarity\", model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# KeyError: \"Unknown task sentence-similarity, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"\n",
    "# Home work - Resolve above error in pipeline method - sentence-similarity unknown task\n",
    "# https://github.com/huggingface/transformers/issues/22923\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461be218-09d2-4711-a618-b74ecc946ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Try tokenizer & embedding in AWS\n",
    "bert base - Text classification\n",
    "Embedding --- Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251191b4-fdf0-4b61-aff9-e20e48c626d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embedding model\n",
    "# encode a text sent\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5bdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.67963091e-02,  2.21861769e-02,  8.42624679e-02,  3.75571772e-02,\n",
       "       -3.82445455e-02, -6.75715208e-02,  4.60077263e-02, -1.17184920e-03,\n",
       "       -6.84570521e-02,  2.96438411e-02, -4.01809700e-02,  2.20640600e-02,\n",
       "       -1.89846586e-02, -4.23013642e-02,  4.09290791e-02,  1.35110617e-02,\n",
       "        8.01860392e-02, -8.45300928e-02, -1.39725834e-01,  1.97063410e-03,\n",
       "       -1.91324037e-02,  2.25233138e-02, -2.30792575e-02,  4.61149514e-02,\n",
       "       -6.00929260e-02, -4.14484814e-02, -8.36759340e-03,  9.64056607e-03,\n",
       "        1.11135775e-02, -4.94560525e-02, -1.15521466e-02,  2.27970853e-02,\n",
       "       -4.45168689e-02, -4.32642177e-03, -1.55703500e-02, -2.07592696e-02,\n",
       "       -1.54289892e-02, -1.51770458e-01, -8.48537497e-03, -7.98861403e-03,\n",
       "       -5.39035024e-03, -4.69117872e-02, -5.75667201e-03, -1.63865145e-02,\n",
       "        9.60722566e-02, -6.71066120e-02,  1.08989440e-02,  2.60103438e-02,\n",
       "        1.03860795e-01,  4.15370576e-02, -1.23352826e-01, -4.81654145e-02,\n",
       "       -1.10043436e-02,  1.04375994e-02,  5.10314368e-02,  2.82243509e-02,\n",
       "       -5.95132336e-02,  1.93364974e-02,  8.57047811e-02, -6.10243976e-02,\n",
       "       -6.71172887e-03,  4.40522097e-02, -7.63966218e-02, -2.03490797e-02,\n",
       "        8.03057104e-02, -5.05053177e-02, -5.92556484e-02, -6.00383319e-02,\n",
       "       -2.14399919e-02, -5.70948310e-02, -1.17369741e-02, -3.32829207e-02,\n",
       "        1.32687157e-02,  1.31624816e-02,  1.15572521e-02, -8.34638923e-02,\n",
       "        2.86669191e-02,  2.75409762e-02,  5.75941876e-02,  4.06908654e-02,\n",
       "        4.63911556e-02, -5.60525693e-02,  1.90393385e-02,  1.56228999e-02,\n",
       "       -5.34013249e-02, -4.39009331e-02,  4.57769483e-02,  4.75180559e-02,\n",
       "       -3.95232514e-02, -9.61243175e-03, -6.99494481e-02,  1.07051238e-01,\n",
       "       -1.05175807e-03,  1.87216117e-03, -7.23862601e-03, -3.48650850e-02,\n",
       "        1.43171489e-01, -5.32556325e-04, -9.47503299e-02,  1.74543902e-01,\n",
       "        2.78157685e-02,  9.19572040e-02,  1.52193550e-02,  2.31096684e-03,\n",
       "       -1.23570280e-04,  6.70182928e-02, -6.64605573e-02,  7.79298171e-02,\n",
       "       -1.78449019e-03, -1.20397424e-02,  3.30518931e-02,  1.75428633e-02,\n",
       "        2.19387305e-03,  2.67138630e-02,  6.78285435e-02,  2.14247629e-02,\n",
       "        3.04026925e-03,  6.89496845e-02,  3.12312320e-02, -4.80596274e-02,\n",
       "        4.33028163e-03, -1.03466213e-03,  1.27227884e-02, -2.66732592e-02,\n",
       "       -5.22095487e-02,  4.65497375e-02,  2.26238631e-02, -2.59221833e-33,\n",
       "        7.83235282e-02, -2.51440182e-02,  5.39948195e-02,  7.68861994e-02,\n",
       "       -1.02173962e-01, -4.01690081e-02, -1.94710325e-02, -3.49799730e-02,\n",
       "        3.28762121e-02,  1.53243504e-02,  2.27043498e-02, -2.05717720e-02,\n",
       "       -5.95505908e-02, -7.63408979e-03, -2.80236937e-02,  9.23349615e-03,\n",
       "       -1.33588240e-02,  7.11903349e-02,  2.21017543e-02,  1.31847695e-01,\n",
       "       -4.65520918e-02, -8.37665349e-02, -1.01482095e-02,  5.80576509e-02,\n",
       "        6.27171472e-02, -2.85472721e-03, -2.57956889e-02, -1.26184896e-01,\n",
       "       -4.08008918e-02,  3.89877111e-02, -2.68060598e-03,  1.71067752e-02,\n",
       "        3.74773405e-02, -1.28683988e-02,  1.86548512e-02, -1.31644299e-02,\n",
       "        6.65958179e-03, -1.23323202e-02, -5.38483821e-03,  5.53258881e-02,\n",
       "       -5.66578191e-03, -2.54064938e-03,  7.46919811e-02, -6.32576570e-02,\n",
       "        1.31167481e-02, -2.90710684e-02, -3.98042314e-02,  2.82759164e-02,\n",
       "        2.67388299e-03, -5.11201099e-02, -7.08411857e-02, -6.75350893e-03,\n",
       "       -9.52850059e-02,  2.65557710e-02, -8.85520596e-03, -3.88051681e-02,\n",
       "        1.53578054e-02, -2.01802906e-02,  4.42123190e-02,  5.42034954e-02,\n",
       "        7.52566978e-02,  1.12141736e-01, -5.15688360e-02, -3.32908779e-02,\n",
       "       -1.67877734e-01, -4.84904125e-02,  1.10788541e-02, -2.53200028e-02,\n",
       "        6.59603328e-02, -9.30086821e-02, -2.55263573e-03, -2.39928700e-02,\n",
       "        2.65126713e-02,  6.66622296e-02,  3.87793146e-02,  3.83651257e-02,\n",
       "       -9.14972462e-03, -3.38463411e-02,  8.62897560e-03, -4.73564453e-02,\n",
       "        1.77375576e-03,  7.55967200e-02, -1.84281822e-02, -1.21507067e-02,\n",
       "        1.58546008e-02, -8.18085577e-03, -5.64631112e-02, -6.64087608e-02,\n",
       "       -4.30916920e-02, -9.75125656e-03, -6.10007867e-02,  3.72557454e-02,\n",
       "        7.57417902e-02, -5.61479442e-02, -1.68360174e-02,  1.62490696e-33,\n",
       "        7.94454291e-02,  4.53502648e-02, -6.99919984e-02,  1.60147424e-03,\n",
       "       -1.02736726e-01, -3.00066508e-02, -2.32277811e-02,  9.47771743e-02,\n",
       "       -6.43399172e-03,  4.19142172e-02,  8.16854984e-02,  8.48785508e-03,\n",
       "        4.53017354e-02,  1.71474717e-03,  6.14162944e-02,  7.35349255e-03,\n",
       "        9.01588798e-02,  2.65798997e-02, -3.94536145e-02,  1.40970256e-02,\n",
       "       -6.70095161e-02,  6.64742962e-02, -3.77787985e-02, -1.31360106e-02,\n",
       "       -2.16929577e-02, -2.67809238e-02, -7.89776631e-03,  7.15505853e-02,\n",
       "       -5.11978194e-02, -4.61491421e-02,  4.34819981e-02, -5.09419963e-02,\n",
       "       -2.99638603e-02,  1.09733880e-01,  7.30039477e-02,  1.94689352e-02,\n",
       "       -2.39793640e-02, -8.51194412e-02, -5.79354763e-02, -7.38960579e-02,\n",
       "       -6.49989024e-02,  6.13630675e-02, -4.37186956e-02,  6.54300079e-02,\n",
       "        8.66448879e-02, -5.29459156e-02,  5.93786165e-02,  9.30074882e-03,\n",
       "       -3.27777341e-02,  4.54276279e-02, -2.73977872e-03, -2.23113932e-02,\n",
       "       -8.26095871e-04,  2.90380567e-02, -1.65859528e-03,  4.04221602e-02,\n",
       "        1.32827321e-02,  8.42767826e-04,  1.29712047e-02, -3.29565676e-03,\n",
       "        8.81135277e-03,  3.75980772e-02, -6.42897934e-03,  6.58438802e-02,\n",
       "        7.14016259e-02,  3.07271425e-02, -2.11982317e-02, -1.58876404e-02,\n",
       "       -8.99042636e-02, -4.18119095e-02, -7.04344288e-02,  8.11399706e-03,\n",
       "        9.69212130e-03, -2.24586446e-02,  9.63099394e-03,  9.75055061e-03,\n",
       "        4.29962985e-02,  2.80329324e-02, -7.73895998e-03,  8.52514245e-03,\n",
       "       -2.21617464e-02, -3.97680793e-03,  2.79847886e-02, -4.76004258e-02,\n",
       "       -1.11504830e-01, -5.78853488e-02,  6.40271604e-02,  2.34090276e-02,\n",
       "        4.93504032e-02,  1.18150683e-02,  2.73292307e-02,  5.33528961e-02,\n",
       "        1.89940874e-02, -1.30727387e-03,  5.34191392e-02, -1.60152052e-08,\n",
       "        6.45849928e-02, -3.32069993e-02,  8.40788335e-02,  8.25397596e-02,\n",
       "        3.65792140e-02,  8.42075273e-02, -2.27826145e-02, -1.30894184e-02,\n",
       "       -8.33791215e-03, -4.77767847e-02,  5.40531613e-02,  1.13148689e-02,\n",
       "       -7.24385232e-02, -9.62578580e-02,  1.79550219e-02, -5.24508506e-02,\n",
       "        7.91400373e-02,  1.35547236e-01, -7.47501180e-02, -1.08539656e-01,\n",
       "        5.65750524e-02, -1.12466766e-02, -5.09790005e-03,  5.45145012e-02,\n",
       "       -1.53180249e-02,  3.93883139e-02, -1.05421257e-03,  4.00769105e-03,\n",
       "       -1.04262568e-02,  5.92204742e-02,  3.42125744e-02,  1.47876561e-01,\n",
       "        1.92405377e-02,  2.37306114e-03,  1.68834906e-02, -3.81775461e-02,\n",
       "       -1.16820098e-03, -4.68999194e-03,  6.78309575e-02,  4.88278456e-03,\n",
       "       -3.22787464e-02, -1.20808790e-02, -8.98968726e-02, -1.86758731e-02,\n",
       "       -5.14561776e-03, -1.64770391e-02,  4.79411595e-02, -7.63858259e-02,\n",
       "        2.96268836e-02, -2.27851588e-02, -6.66516945e-02,  3.17042554e-03,\n",
       "        6.85701668e-02,  6.75765350e-02,  2.09327098e-02,  7.30853975e-02,\n",
       "       -1.48665830e-02,  6.49797171e-02, -2.52496842e-02,  9.20014363e-03,\n",
       "        3.60636525e-02, -1.99628547e-02, -3.19879949e-02, -2.72671762e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi How are you\"\n",
    "model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dacb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: boto3 1.34.95\n",
      "Uninstalling boto3-1.34.95:\n",
      "  Successfully uninstalled boto3-1.34.95\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall boto3 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4238fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82806dcd82f4428b0bba10a6fef90f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9502fd8a29034b15996eef97b743145c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8606d4d3ffeb4bf9accd2e0d14552172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b75743964724d9b9ad7b9720f5cf773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f96e2226244c5cb0a27010d4327c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/778 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ed81ccca6348a4b5012934557dc206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f887d787cb5343f6988b43a882e71b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748bcbe015d84c20a434e301033056ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d504d917589447cc9a817709726ea0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c703f3f2f649a1936c951c662973b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7322a003ac4b77bbc2d21a039496fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3506e2d88d492188c50d4c92cd373e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "question = \"<Q> How many models can I host on Hugging Face?\"\n",
    "answer_1 = \"<A> All plans come with unlimited models and datasets.\"\n",
    "answer_2 = \"<A> AutoNLP is an automatic way to train and deploy state of the art NLP models, seamlessly integrated with the Hugging Face ecosystem.\"\n",
    "answer_3 = \"<A> Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.\"\n",
    "\n",
    "model = SentenceTransformer(\"clips/mfaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56062538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (768,)\n",
      "<class 'numpy.ndarray'> (3, 768)\n",
      "[[{'corpus_id': 0, 'score': 0.5535222291946411}, {'corpus_id': 2, 'score': 0.5052736401557922}, {'corpus_id': 1, 'score': 0.47080811858177185}]]\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = model.encode(question)\n",
    "corpus_embeddings = model.encode([answer_1, answer_2, answer_3])\n",
    "print(type(query_embeddings),query_embeddings.shape)\n",
    "print(type(corpus_embeddings),corpus_embeddings.shape)\n",
    "print(util.semantic_search(query_embeddings,corpus_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cec2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delete the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1496140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 4426,    2],\n",
       "         [   0, 2396,    2],\n",
       "         [   0,  977,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,  572,    2],\n",
       "         [   0,   36,    2],\n",
       "         [   0,  148,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,  347,    2],\n",
       "         [   0,   10,    2],\n",
       "         [   0,  653,    2],\n",
       "         [   0,  113,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,  347,    2],\n",
       "         [   0,   36,    2],\n",
       "         [   0,  104,    2],\n",
       "         [   0,   28,    2],\n",
       "         [   0,   96,    2],\n",
       "         [   0,   91,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,  501,    2],\n",
       "         [   0,   10,    2],\n",
       "         [   0,  653,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,   87,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0, 1096,    2],\n",
       "         [   0,   36,    2],\n",
       "         [   0,   91,    2],\n",
       "         [   0,  808,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,   36,    2],\n",
       "         [   0,  653,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,  572,    2],\n",
       "         [   0,   75,    2],\n",
       "         [   0,  706,    2],\n",
       "         [   0,  706,    2],\n",
       "         [   0,   17,    2],\n",
       "         [   0,  653,    2],\n",
       "         [   0,  706,    2],\n",
       "         [   0,    2,    1],\n",
       "         [   0,  563,    2],\n",
       "         [   0,   10,    2],\n",
       "         [   0,  501,    2],\n",
       "         [   0,   28,    2],\n",
       "         [   0,  705,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_tokens = model.tokenize(question)\n",
    "quest_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69701142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5535, 0.4708, 0.5053]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "pytorch_cos_sim(query_embeddings, corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc673b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m cos_sim\n\u001b[1;32m      2\u001b[0m cos_sim(query_embeddings, corpus_embeddings)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "from util import cos_sim\n",
    "cos_sim(query_embeddings, corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ea0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
