{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzOIlKzMycyR7TTT5IMLKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "655e97a337174987b5e383d44d7844e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b375abb96c8b4fec9a616ec69c3af5e5",
              "IPY_MODEL_48a8eb97461648f0937009c0302a9f88",
              "IPY_MODEL_abbdd5ef02594660a7e4457f500cead5"
            ],
            "layout": "IPY_MODEL_593d5cb421d5494a9e0ea7976cb7a8af"
          }
        },
        "b375abb96c8b4fec9a616ec69c3af5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f3dc238b26d4b3ba0b1580b14d9cf0d",
            "placeholder": "​",
            "style": "IPY_MODEL_89a2486be1d14bc8b3396959ce27c91d",
            "value": "config.json: 100%"
          }
        },
        "48a8eb97461648f0937009c0302a9f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cb14044b8ff4864b348b0ae351685fa",
            "max": 715,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6f7ea075a2a43f3b22a1b3769cadc68",
            "value": 715
          }
        },
        "abbdd5ef02594660a7e4457f500cead5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54764ffdde8549a59d7b573cf5011dbe",
            "placeholder": "​",
            "style": "IPY_MODEL_f3934d6fd3ed4e418264c4c8d33380b4",
            "value": " 715/715 [00:00&lt;00:00, 44.9kB/s]"
          }
        },
        "593d5cb421d5494a9e0ea7976cb7a8af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3dc238b26d4b3ba0b1580b14d9cf0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89a2486be1d14bc8b3396959ce27c91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cb14044b8ff4864b348b0ae351685fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6f7ea075a2a43f3b22a1b3769cadc68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54764ffdde8549a59d7b573cf5011dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3934d6fd3ed4e418264c4c8d33380b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92a6156bf5c246bea05dc65416703b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e30755bfceb24aaf98f75cccd68f8048",
              "IPY_MODEL_6c12d8bfdd9c43b98c4f4536160ce221",
              "IPY_MODEL_082591fd02cc4a52b96bbe0b33a9e989"
            ],
            "layout": "IPY_MODEL_7953a529146f4fb09bc645ccdbfbc6b2"
          }
        },
        "e30755bfceb24aaf98f75cccd68f8048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65429ad874b44d378ea635d7f22b3eae",
            "placeholder": "​",
            "style": "IPY_MODEL_cadb1a0c6158428da74dcca8ebed8784",
            "value": "model.safetensors: 100%"
          }
        },
        "6c12d8bfdd9c43b98c4f4536160ce221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a1e6a10686b42e8ae9db3eac929b0f3",
            "max": 3444848602,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f9445025f004c169d531f2d102cc15f",
            "value": 3444848602
          }
        },
        "082591fd02cc4a52b96bbe0b33a9e989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21d901221324c73bff29be1fe1d8e1d",
            "placeholder": "​",
            "style": "IPY_MODEL_cd278930a95641f7bd645044b730d537",
            "value": " 3.44G/3.44G [00:24&lt;00:00, 158MB/s]"
          }
        },
        "7953a529146f4fb09bc645ccdbfbc6b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65429ad874b44d378ea635d7f22b3eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cadb1a0c6158428da74dcca8ebed8784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1e6a10686b42e8ae9db3eac929b0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f9445025f004c169d531f2d102cc15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d21d901221324c73bff29be1fe1d8e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd278930a95641f7bd645044b730d537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pj2111/Assignments/blob/master/assignment_data/warmup_model_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8pvExioUJUIb"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes torch transformers huggingface_hub peft accelerate quanto > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Floating point data types have a feature called precision. Precision is defined as  the number of bits used in memory to store a number, and the number of digits after the decimal point that can be stored before rounding.\n",
        "\n",
        "The higher the precision, bigger the size of the memory and larger the models will become. The max precision in PyTorch is 64-bit. Models usually are trained with 32-bit precision and quantised from there.\n",
        "\n",
        "Quantising is changing the precision of the parameters used in the model. In short reducing the size of the parameters, and in turn reducing the overall model size. This reduction will lead to loss in prediction accuracy, speed and overall usefulness of the model.\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/quantization\n",
        "\n",
        "The above link dives into the comparison, and provides handson with different libraries like AQLM, PEFT and BitsandBytes. *We will use this doc for our review below*\n",
        "\n",
        "Purpose of BitsAndBytes library and its supporting PEFT library is to load models of bigger size into smaller GPU VRAM for Fine-Tuning. Apart from space, it provides better error management too.\n",
        "\n",
        "https://huggingface.co/docs/bitsandbytes/optimizers\n",
        "\n",
        "Fine-Tuning bigger models will require bigger dataset, for the parameters to be learnt.\n",
        "\n",
        "In this notebook, we are discussing only loading the model for inference. We also review the model loading size with & w/o quantising\n",
        "\n",
        "### Reading Homework:\n",
        "\n",
        "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "https://huggingface.co/blog/hf-bitsandbytes-integration"
      ],
      "metadata": {
        "id": "Sr3jwtZUKQFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Intro to Quanto"
      ],
      "metadata": {
        "id": "I45oNQRKNw9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig"
      ],
      "metadata": {
        "id": "LoyCuLWBTPRQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"facebook/opt-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "quantization_config = QuantoConfig(weights=\"int8\")\n",
        "\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                       device_map=\"cuda:0\",\n",
        "                                                       quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "ct5UHHizNfWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.get_memory_footprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dLOs7QpOIqm",
        "outputId": "30de26d7-eb6f-4ab5-9f28-126b771ef327"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500957760"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like quanto there are other libraries that provide quantisation support\n",
        "\n",
        "- AQLM\n",
        "\n",
        "- AWQ\n",
        "\n",
        "- AutoGPTQ\n",
        "\n",
        "- ExLLama  \n",
        "\n",
        "All above have specific usecases with specific models. Its mostly not used in production unless that specific model is used, and the resources are an issue. BitsandBytes will be covered in detail below"
      ],
      "metadata": {
        "id": "D7d8O233OTyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuration option of BitsAndBytesConfig\n",
        "\n",
        "load_in_8bit (bool, optional, defaults to False) — This flag is used to enable 8-bit quantization with LLM.int8().\n",
        "\n",
        "load_in_4bit (bool, optional, defaults to False)* — This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from bitsandbytes.\n",
        "\n",
        "llm_int8_threshold (float, optional, defaults to 6.0) — This corresponds to the outlier threshold for outlier detection as described in LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale paper: https://arxiv.org/abs/2208.07339 Any hidden states value that is above this threshold will be considered an outlier and the operation on those values will be done in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but there are some exceptional systematic outliers that are very differently distributed for large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6, but a lower threshold might be needed for more unstable models (small models, fine-tuning).\n",
        "\n",
        "llm_int8_skip_modules (List[str], optional) — An explicit list of the modules\n",
        "that we do not want to convert in 8-bit. This is useful for models such as Jukebox that has several heads in different places and not necessarily at the last position. For example for CausalLM models, the last lm_head is kept in its original dtype.\n",
        "\n",
        "llm_int8_enable_fp32_cpu_offload (bool, optional, defaults to False) — This flag is used for advanced use cases and users that are aware of this feature. If you want to split your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use this flag. This is useful for offloading large models such as google/flan-t5-xxl. Note that the int8 operations will not be run on CPU.\n",
        "\n",
        "llm_int8_has_fp16_weight (bool, optional, defaults to False) — This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not have to be converted back and forth for the backward pass.\n",
        "\n",
        "bnb_4bit_compute_dtype (torch.dtype or str, optional, defaults to torch.float32) — This sets the computational type which might be different than the input type. For example, inputs might be fp32, but computation can be set to bf16 for speedups.\n",
        "\n",
        "bnb_4bit_quant_type (str, optional, defaults to \"fp4\") — This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types which are specified by fp4 or nf4.\n",
        "bnb_4bit_use_double_quant (bool, optional, defaults to False) — This flag is used for nested quantization where the quantization constants from the first quantization are quantized again.\n",
        "\n",
        "bnb_4bit_quant_storage (torch.dtype or str, optional, defaults to torch.uint8) — This sets the storage type to pack the quanitzed 4-bit prarams.\n",
        "kwargs (Dict[str, Any], optional) — Additional parameters from which to initialize the configuration object.\n"
      ],
      "metadata": {
        "id": "spMRPLadRN0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# loading model in 4-bit, ensure bitsandbytes, accelerate is installed"
      ],
      "metadata": {
        "id": "vWfILQ8GTTPH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,)\n",
        "\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\",\n",
        "                                                  device_map=\"auto\",\n",
        "                                                  quantization_config=quantization_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "655e97a337174987b5e383d44d7844e9",
            "b375abb96c8b4fec9a616ec69c3af5e5",
            "48a8eb97461648f0937009c0302a9f88",
            "abbdd5ef02594660a7e4457f500cead5",
            "593d5cb421d5494a9e0ea7976cb7a8af",
            "2f3dc238b26d4b3ba0b1580b14d9cf0d",
            "89a2486be1d14bc8b3396959ce27c91d",
            "4cb14044b8ff4864b348b0ae351685fa",
            "d6f7ea075a2a43f3b22a1b3769cadc68",
            "54764ffdde8549a59d7b573cf5011dbe",
            "f3934d6fd3ed4e418264c4c8d33380b4",
            "92a6156bf5c246bea05dc65416703b1f",
            "e30755bfceb24aaf98f75cccd68f8048",
            "6c12d8bfdd9c43b98c4f4536160ce221",
            "082591fd02cc4a52b96bbe0b33a9e989",
            "7953a529146f4fb09bc645ccdbfbc6b2",
            "65429ad874b44d378ea635d7f22b3eae",
            "cadb1a0c6158428da74dcca8ebed8784",
            "2a1e6a10686b42e8ae9db3eac929b0f3",
            "7f9445025f004c169d531f2d102cc15f",
            "d21d901221324c73bff29be1fe1d8e1d",
            "cd278930a95641f7bd645044b730d537"
          ]
        },
        "id": "y_1gVS5sOTB3",
        "outputId": "837465b1-4bd9-4ad0-9420-867402d65eef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "655e97a337174987b5e383d44d7844e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92a6156bf5c246bea05dc65416703b1f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4bit.get_memory_footprint()  # 3.4 GB model is loaded with 1.6 GB Space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bePPrHy0PTDV",
        "outputId": "844e6578-e823-48b2-bf38-cece961403e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1632878592"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model in 8-bit\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True,)\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "-DDRhqB0NfUJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8bit.get_memory_footprint()  # 2.2 GB memory usage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foLtJ5_QNfRp",
        "outputId": "37f5f4e5-702e-4fbb-82d1-29afe737d29c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2236858368"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
        "\n",
        "device_map = {\n",
        "    \"transformer.word_embeddings\": 0,\n",
        "    \"transformer.word_embeddings_layernorm\": 0,\n",
        "    \"lm_head\": \"cpu\",\n",
        "    \"transformer.h\": 0,\n",
        "    \"transformer.ln_f\": 0,\n",
        "}"
      ],
      "metadata": {
        "id": "91j_4BYTNfPS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-1b7\",\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGTe-la0R47N",
        "outputId": "767738b0-ed55-4956-a044-727efa52a8b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:Tied parameters are on different devices: {'lm_head.weight': 'cpu', 'transformer.word_embeddings.weight': 0}. Please modify your custom device map or set `device_map='auto'`. \n",
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An “outlier” is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed ([-3.5, 3.5]), this distribution can be very different for large models ([-60, 6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that, there is a significant performance penalty."
      ],
      "metadata": {
        "id": "4WEdceaASHg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"bigscience/bloom-1b7\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    llm_int8_threshold=10.0,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        ")\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q69_VE5jR4x2",
        "outputId": "0eeccda4-15e5-490d-e236-c6b062f0c5e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:Tied parameters are on different devices: {'lm_head.weight': 'cpu', 'transformer.word_embeddings.weight': 0}. Please modify your custom device map or set `device_map='auto'`. \n",
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models like Jukebox, you don’t need to quantize every module to 8-bit which can actually cause instability. With Jukebox, there are several lm_head modules that should be skipped using the llm_int8_skip_modules parameter"
      ],
      "metadata": {
        "id": "A5y6w70wSd4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"bigscience/bloom-1b7\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    llm_int8_skip_modules=[\"lm_head\"],\n",
        ")\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "id": "qGmwik6wQyY5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if the model is to be loaded for fine-tuning then the compute type can be moded\n",
        "import torch\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")"
      ],
      "metadata": {
        "id": "mtPqeM6gQyWU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an addition 0.4 bits/parameter."
      ],
      "metadata": {
        "id": "McLzLXvZS6eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "4PC20z-4TayT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The cell might crash, as there will be some memory isseu\n",
        "\n",
        "double_quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b\", quantization_config=double_quant_config)"
      ],
      "metadata": {
        "id": "K5kWSlVzQySj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73VoS-99QyQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOoKplwRQyNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhRGzz2sQyJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMBfroAENfMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import init_empty_weights\n",
        "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
        "from mingpt.model import GPT\n",
        "\n",
        "model_config = GPT.get_default_config()\n",
        "model_config.model_type = 'gpt2-xl'\n",
        "model_config.vocab_size = 50257\n",
        "model_config.block_size = 1024\n",
        "\n",
        "with init_empty_weights():\n",
        "    empty_model = GPT(model_config)\n",
        "\n",
        "bnb_quantization_config = BnbQuantizationConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16,  # optional\n",
        "  bnb_4bit_use_double_quant=True,         # optional\n",
        "  bnb_4bit_quant_type=\"nf4\"               # optional\n",
        ")\n",
        "\n",
        "quantized_model = load_and_quantize_model(\n",
        "  empty_model,\n",
        "  weights_location=weights_location,\n",
        "  bnb_quantization_config=bnb_quantization_config,\n",
        "  device_map = \"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "P5kbYDzBKN7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YX3NjhbKJvZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjhbLm5dJv-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MV-itGzPJwDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcaJrzBkJwHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnCh2VfzJwKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_oe9tmkJwOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi8xI1lzJwSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}