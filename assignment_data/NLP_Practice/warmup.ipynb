{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = '/Users/apple/Desktop/KamalRaj/Assignments/assignment_data/Movies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffMovie ID,Movie Title,Release Year,Runtime,Budget,Box Office\\n1,Harry Potter and the Philosopher\\'s Stone,2001,152,\"$125,000,000 \",\"$1,002,000,000 \"\\n2,Harry Potter and the Chamber of Secrets,2002,161,\"$100,000,000 \",\"$880,300,000 \"\\n3,Harry Potter and the Prisoner of Azkaban,2004,142,\"$130,000,000 \",\"$796,700,000 \"\\n4,Harry Potter and the Goblet of Fire,2005,157,\"$150,000,000 \",\"$896,400,000 \"\\n5,Harry Potter and the Order of the Phoenix,2007,138,\"$150,000,000 \",\"$942,000,000 \"\\n6,Harry Potter and the Half-Blood Prince,2009,153,\"$250,000,000 \",\"$943,200,000 \"\\n7,Harry Potter and the Deathly Hallows Part 1,2010,146,\"$200,000,000 \",\"$976,900,000 \"\\n8,Harry Potter and the Deathly Hallows Part 2,2011,130,\"$250,000,000 \",\"$1,342,000,000 \"\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(movies_path, 'r') as f:\n",
    "    text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeffMovie ID',\n",
       "  'Movie Title',\n",
       "  'Release Year',\n",
       "  'Runtime',\n",
       "  'Budget',\n",
       "  'Box Office'],\n",
       " ['1',\n",
       "  \"Harry Potter and the Philosopher's Stone\",\n",
       "  '2001',\n",
       "  '152',\n",
       "  '\"$125',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$1',\n",
       "  '002',\n",
       "  '000',\n",
       "  '000 \"'],\n",
       " ['2',\n",
       "  'Harry Potter and the Chamber of Secrets',\n",
       "  '2002',\n",
       "  '161',\n",
       "  '\"$100',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$880',\n",
       "  '300',\n",
       "  '000 \"'],\n",
       " ['3',\n",
       "  'Harry Potter and the Prisoner of Azkaban',\n",
       "  '2004',\n",
       "  '142',\n",
       "  '\"$130',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$796',\n",
       "  '700',\n",
       "  '000 \"'],\n",
       " ['4',\n",
       "  'Harry Potter and the Goblet of Fire',\n",
       "  '2005',\n",
       "  '157',\n",
       "  '\"$150',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$896',\n",
       "  '400',\n",
       "  '000 \"'],\n",
       " ['5',\n",
       "  'Harry Potter and the Order of the Phoenix',\n",
       "  '2007',\n",
       "  '138',\n",
       "  '\"$150',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$942',\n",
       "  '000',\n",
       "  '000 \"'],\n",
       " ['6',\n",
       "  'Harry Potter and the Half-Blood Prince',\n",
       "  '2009',\n",
       "  '153',\n",
       "  '\"$250',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$943',\n",
       "  '200',\n",
       "  '000 \"'],\n",
       " ['7',\n",
       "  'Harry Potter and the Deathly Hallows Part 1',\n",
       "  '2010',\n",
       "  '146',\n",
       "  '\"$200',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$976',\n",
       "  '900',\n",
       "  '000 \"'],\n",
       " ['8',\n",
       "  'Harry Potter and the Deathly Hallows Part 2',\n",
       "  '2011',\n",
       "  '130',\n",
       "  '\"$250',\n",
       "  '000',\n",
       "  '000 \"',\n",
       "  '\"$1',\n",
       "  '342',\n",
       "  '000',\n",
       "  '000 \"'],\n",
       " ['']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = []\n",
    "for i in text.split('\\n'):\n",
    "    l1.append(i.split(','))\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object attribute 'keys' is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dict1 \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 2\u001b[0m dict1\u001b[39m.\u001b[39;49mkeys \u001b[39m=\u001b[39m l1[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m dict1\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object attribute 'keys' is read-only"
     ]
    }
   ],
   "source": [
    "dict1 = {}\n",
    "dict1.keys = l1[0] # doubt\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Box Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>2001</td>\n",
       "      <td>152</td>\n",
       "      <td>$125,000,000</td>\n",
       "      <td>$1,002,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>2002</td>\n",
       "      <td>161</td>\n",
       "      <td>$100,000,000</td>\n",
       "      <td>$880,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>2004</td>\n",
       "      <td>142</td>\n",
       "      <td>$130,000,000</td>\n",
       "      <td>$796,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>2005</td>\n",
       "      <td>157</td>\n",
       "      <td>$150,000,000</td>\n",
       "      <td>$896,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>2007</td>\n",
       "      <td>138</td>\n",
       "      <td>$150,000,000</td>\n",
       "      <td>$942,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movie ID                                Movie Title  Release Year  Runtime  \\\n",
       "0         1   Harry Potter and the Philosopher's Stone          2001      152   \n",
       "1         2    Harry Potter and the Chamber of Secrets          2002      161   \n",
       "2         3   Harry Potter and the Prisoner of Azkaban          2004      142   \n",
       "3         4        Harry Potter and the Goblet of Fire          2005      157   \n",
       "4         5  Harry Potter and the Order of the Phoenix          2007      138   \n",
       "\n",
       "          Budget       Box Office  \n",
       "0  $125,000,000   $1,002,000,000   \n",
       "1  $100,000,000     $880,300,000   \n",
       "2  $130,000,000     $796,700,000   \n",
       "3  $150,000,000     $896,400,000   \n",
       "4  $150,000,000     $942,000,000   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(movies_path) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Box Office</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>2001</td>\n",
       "      <td>152</td>\n",
       "      <td>$125,000,000</td>\n",
       "      <td>$1,002,000,000</td>\n",
       "      <td>Box office collection of movie - Harry Potter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>2002</td>\n",
       "      <td>161</td>\n",
       "      <td>$100,000,000</td>\n",
       "      <td>$880,300,000</td>\n",
       "      <td>Box office collection of movie - Harry Potter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>2004</td>\n",
       "      <td>142</td>\n",
       "      <td>$130,000,000</td>\n",
       "      <td>$796,700,000</td>\n",
       "      <td>Box office collection of movie - Harry Potter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>2005</td>\n",
       "      <td>157</td>\n",
       "      <td>$150,000,000</td>\n",
       "      <td>$896,400,000</td>\n",
       "      <td>Box office collection of movie - Harry Potter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>2007</td>\n",
       "      <td>138</td>\n",
       "      <td>$150,000,000</td>\n",
       "      <td>$942,000,000</td>\n",
       "      <td>Box office collection of movie - Harry Potter ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movie ID                                Movie Title  Release Year  Runtime  \\\n",
       "0         1   Harry Potter and the Philosopher's Stone          2001      152   \n",
       "1         2    Harry Potter and the Chamber of Secrets          2002      161   \n",
       "2         3   Harry Potter and the Prisoner of Azkaban          2004      142   \n",
       "3         4        Harry Potter and the Goblet of Fire          2005      157   \n",
       "4         5  Harry Potter and the Order of the Phoenix          2007      138   \n",
       "\n",
       "          Budget       Box Office  \\\n",
       "0  $125,000,000   $1,002,000,000    \n",
       "1  $100,000,000     $880,300,000    \n",
       "2  $130,000,000     $796,700,000    \n",
       "3  $150,000,000     $896,400,000    \n",
       "4  $150,000,000     $942,000,000    \n",
       "\n",
       "                                                text  \n",
       "0  Box office collection of movie - Harry Potter ...  \n",
       "1  Box office collection of movie - Harry Potter ...  \n",
       "2  Box office collection of movie - Harry Potter ...  \n",
       "3  Box office collection of movie - Harry Potter ...  \n",
       "4  Box office collection of movie - Harry Potter ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = \"Box office collection of movie - \" + df['Movie Title'] + \" is \" + df['Box Office']\n",
    "# for i, r in df.iterrows():\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af9aade29b43abbb73acbfe4600a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc1b5f1ba184aa28af078e07534cc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7668036b1c46639ef2befb953dfd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8e377ee1e64c42865c22d5d1da03d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998072981834412}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('text-classification')\n",
    "classifier(\" Ramesh is a good boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9982427358627319},\n",
       " {'label': 'POSITIVE', 'score': 0.997869610786438},\n",
       " {'label': 'POSITIVE', 'score': 0.9964237809181213},\n",
       " {'label': 'POSITIVE', 'score': 0.9981662631034851},\n",
       " {'label': 'POSITIVE', 'score': 0.9979549646377563},\n",
       " {'label': 'POSITIVE', 'score': 0.9974629878997803},\n",
       " {'label': 'POSITIVE', 'score': 0.9977529644966125},\n",
       " {'label': 'POSITIVE', 'score': 0.9982642531394958}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffMovie ID,Movie Title,Release Year,Runtime,Budget,Box Office\\n1,Harry Potter and the Philosopher\\'s Stone,2001,152,\"$125,000,000 \",\"$1,002,000,000 \"\\n2,Harry Potter and the Chamber of Secrets,2002,161,\"$100,000,000 \",\"$880,300,000 \"\\n3,Harry Potter and the Prisoner of Azkaban,2004,142,\"$130,000,000 \",\"$796,700,000 \"\\n4,Harry Potter and the Goblet of Fire,2005,157,\"$150,000,000 \",\"$896,400,000 \"\\n5,Harry Potter and the Order of the Phoenix,2007,138,\"$150,000,000 \",\"$942,000,000 \"\\n6,Harry Potter and the Half-Blood Prince,2009,153,\"$250,000,000 \",\"$943,200,000 \"\\n7,Harry Potter and the Deathly Hallows Part 1,2010,146,\"$200,000,000 \",\"$976,900,000 \"\\n8,Harry Potter and the Deathly Hallows Part 2,2011,130,\"$250,000,000 \",\"$1,342,000,000 \"\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline ==> auto tokenizer + automodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_1 = tokenizer(\"Ramesh is a good boy\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tok_1\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "tok_1.to('cuda')\n",
    "# AssertionError: Torch not compiled with CUDA enabled\n",
    "# Above error is thrown when cuda is not an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 8223, 9953, 2003, 1037, 2204, 2879,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tok_1)\n",
    "type(tok_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 8223, 9953, 2003, 1037, 2204, 2879,  102])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(tok_1['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'input_ids' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtok_1)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'input_ids' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "print(**tok_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.0886,  4.4654]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tok_1)\n",
    "# model(\"Ramesh is a boy\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0886,  4.4654]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = output.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(logits.argmax())\n",
    "print(logits.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")              \n",
    "# instance of tokenizer object which takes in  input as text\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "# instence of model object in--token id(input_id and attention mask)====== output as label id\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# output dict with input_id and attention mask\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "# output of model contains logits and some other outputs so picking only logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "labels = torch.tensor([1])\n",
    "loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(model.config.id2label)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.39.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review how kwargs happen # Home work - why we use ** in (    logits = model(**inputs).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_compile_jinja_template',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'default_chat_template',\n",
       " 'deprecation_warnings',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ram', '##esh', 'is', 'a', 'boy']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Ramesh is a boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to take in 3 args and add them\n",
    "def add_3(a,b,c):\n",
    "   return a+b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2\n",
    "y = 5\n",
    "z = 7\n",
    "\n",
    "add_3(x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_3(a=x,b=y,c=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 5, 'c': 7}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1={'a':x,\n",
    "      'b':y,\n",
    "      'c':z\n",
    "      }\n",
    "dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 5, 'c': 7}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic2 = dict(a=x, b=y, c=z)\n",
    "dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 5, 'c': 7}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_3 = dict(**dic1)\n",
    "dic_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_3(**dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
