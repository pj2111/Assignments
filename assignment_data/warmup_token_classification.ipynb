{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Individual tokens inside the sentences are classified into different types.\n",
    "default_model = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # 1.4GB model\n",
    "# classifier = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tokenizer = AutoTokenizer.from_pretrained(default_model,\n",
    "                                              resume_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O means the word doesn’t correspond to any entity.\n",
    "\n",
    "B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
    "\n",
    "B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
    "\n",
    "B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
    "\n",
    "B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity.\n",
    "\n",
    "O, Outside of a named entity\n",
    "\n",
    "B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "\n",
    "I-MIS, Miscellaneous entity\n",
    "\n",
    "B-PER, Beginning of a person’s name right after another person’s name\n",
    "\n",
    "I-PER, Person’s name\n",
    "\n",
    "B-ORG, Beginning of an organisation right after another organisation\n",
    "\n",
    "I-ORG, Organisation\n",
    "\n",
    "B-LOC, Beginning of a location right after another location\n",
    "\n",
    "I-LOC, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ner_model = AutoModelForTokenClassification.from_pretrained(default_model,\n",
    "                                                            resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">BertConfig <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"_name_or_path\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"dbmdz/bert-large-cased-finetuned-conll03-english\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"_num_labels\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"architectures\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"BertForTokenClassification\"</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_probs_dropout_prob\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"classifier_dropout\"</span>: null,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"directionality\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"bidi\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_act\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gelu\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_dropout_prob\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"id2label\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"0\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"O\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"1\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"B-MISC\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"2\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I-MISC\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"3\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"B-PER\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"4\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I-PER\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"5\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"B-ORG\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"6\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I-ORG\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"7\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"B-LOC\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"8\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I-LOC\"</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"initializer_range\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"intermediate_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"label2id\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"B-LOC\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"B-MISC\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"B-ORG\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"B-PER\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"I-LOC\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"I-MISC\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"I-ORG\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"I-PER\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"O\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"layer_norm_eps\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"model_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"bert\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_attention_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_hidden_layers\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pad_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pooler_fc_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pooler_num_attention_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pooler_num_fc_layers\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pooler_size_per_head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pooler_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"first_token_transform\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"position_embedding_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"absolute\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.39.2\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"type_vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"use_cache\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28996</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "BertConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"_name_or_path\"\u001b[0m: \u001b[32m\"dbmdz/bert-large-cased-finetuned-conll03-english\"\u001b[0m,\n",
       "  \u001b[32m\"_num_labels\"\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "  \u001b[32m\"architectures\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"BertForTokenClassification\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"classifier_dropout\"\u001b[0m: null,\n",
       "  \u001b[32m\"directionality\"\u001b[0m: \u001b[32m\"bidi\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m1024\u001b[0m,\n",
       "  \u001b[32m\"id2label\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"0\"\u001b[0m: \u001b[32m\"O\"\u001b[0m,\n",
       "    \u001b[32m\"1\"\u001b[0m: \u001b[32m\"B-MISC\"\u001b[0m,\n",
       "    \u001b[32m\"2\"\u001b[0m: \u001b[32m\"I-MISC\"\u001b[0m,\n",
       "    \u001b[32m\"3\"\u001b[0m: \u001b[32m\"B-PER\"\u001b[0m,\n",
       "    \u001b[32m\"4\"\u001b[0m: \u001b[32m\"I-PER\"\u001b[0m,\n",
       "    \u001b[32m\"5\"\u001b[0m: \u001b[32m\"B-ORG\"\u001b[0m,\n",
       "    \u001b[32m\"6\"\u001b[0m: \u001b[32m\"I-ORG\"\u001b[0m,\n",
       "    \u001b[32m\"7\"\u001b[0m: \u001b[32m\"B-LOC\"\u001b[0m,\n",
       "    \u001b[32m\"8\"\u001b[0m: \u001b[32m\"I-LOC\"\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m4096\u001b[0m,\n",
       "  \u001b[32m\"label2id\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"B-LOC\"\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "    \u001b[32m\"B-MISC\"\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[32m\"B-ORG\"\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m\"B-PER\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m\"I-LOC\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "    \u001b[32m\"I-MISC\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m\"I-ORG\"\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "    \u001b[32m\"I-PER\"\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "    \u001b[32m\"O\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-12\u001b[0m,\n",
       "  \u001b[32m\"max_position_embeddings\"\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"bert\"\u001b[0m,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m24\u001b[0m,\n",
       "  \u001b[32m\"pad_token_id\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "  \u001b[32m\"pooler_fc_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"pooler_num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"pooler_num_fc_layers\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"pooler_size_per_head\"\u001b[0m: \u001b[1;36m128\u001b[0m,\n",
       "  \u001b[32m\"pooler_type\"\u001b[0m: \u001b[32m\"first_token_transform\"\u001b[0m,\n",
       "  \u001b[32m\"position_embedding_type\"\u001b[0m: \u001b[32m\"absolute\"\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.39.2\"\u001b[0m,\n",
       "  \u001b[32m\"type_vocab_size\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "  \u001b[32m\"use_cache\"\u001b[0m: true,\n",
       "  \u001b[32m\"vocab_size\"\u001b[0m: \u001b[1;36m28996\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ner_model.config\n",
    "from rich import print\n",
    "from transformers import AutoConfig\n",
    "print(AutoConfig.from_pretrained(default_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "\"O\",       # Outside of a named entity\n",
    "\"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "\"I-MISC\",  # Miscellaneous entity\n",
    "\"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "\"I-PER\",   # Person's name\n",
    "\"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "\"I-ORG\",   # Organisation\n",
    "\"B-LOC\",   # Beginning of a location right after another location\n",
    "\"I-LOC\"    # Location \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am from United States and I like playing with data Ramesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'United',\n",
       " 'States',\n",
       " 'and',\n",
       " 'I',\n",
       " 'like',\n",
       " 'playing',\n",
       " 'with',\n",
       " 'data',\n",
       " 'Ram',\n",
       " '##esh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenized = ner_tokenizer.tokenize(sentence)\n",
    "sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   146,  1821,  1121,  1244,  1311,  1105,   146,  1176,  1773,\n",
       "          1114,  2233, 11447, 10654,   102]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_input = ner_tokenizer.encode(sentence, return_tensors='pt')\n",
    "sentence_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0070e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1285e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5390e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0444e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5033e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8817e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0944e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9918e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.8473e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0482e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.5018e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.7410e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.6160e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.1306e-02</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0731e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.4023e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1875e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.3043e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1118e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3193e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.7046e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4368e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.8214e-01</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8959e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4921e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8533e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.1833e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0280e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.5857e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0144e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3723e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0276e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.7403e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1357e-03</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1665e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0278e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.8959e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0961e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.2558e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.2961e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3618e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0690e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0093e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3048e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.8662e+00</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4161e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8178e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.6483e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.2271e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4522e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.2266e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2002e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2413e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.7637e+00</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1036e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4121e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5921e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4863e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3190e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5500e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.4275e-03</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.7939e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.4909e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1186e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3898e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5176e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.5681e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-7.8249e-01</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8102e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.7170e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9194e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-7.1405e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1479e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3559e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2051e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.6351e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0897e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8197e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3955e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8723e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-7.9155e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1569e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4212e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-9.2269e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.7095e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3123e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9065e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5261e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8674e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-9.2102e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1516e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3888e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-8.6533e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.6910e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2451e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9952e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.7431e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8713e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-9.3357e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1226e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3456e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-7.4065e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.8541e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0089e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9645e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.5646e-02</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8419e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-9.9125e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6967e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0559e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-7.4033e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.7475e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.3590e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4282e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2192e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.7365e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8693e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2625e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8687e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.6671e-02</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.6221e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0317e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.2456e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1748e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.9106e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3734e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0070e+01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1285e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5390e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0444e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5033e+00</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8817e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0944e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.9918e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.8473e-01</span><span style=\"font-weight: bold\">]]]</span>,\n",
       "       <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.0070e+01\u001b[0m, \u001b[1;36m-2.1285e+00\u001b[0m, \u001b[1;36m-1.5390e+00\u001b[0m, \u001b[1;36m-2.0444e+00\u001b[0m, \u001b[1;36m-1.5033e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.8817e+00\u001b[0m, \u001b[1;36m-1.0944e+00\u001b[0m, \u001b[1;36m-1.9918e+00\u001b[0m,  \u001b[1;36m5.8473e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.0482e+01\u001b[0m, \u001b[1;36m-2.5018e+00\u001b[0m, \u001b[1;36m-1.7410e+00\u001b[0m, \u001b[1;36m-2.6160e+00\u001b[0m, \u001b[1;36m-6.1306e-02\u001b[0m,\n",
       "          \u001b[1;36m-2.0731e+00\u001b[0m,  \u001b[1;36m3.4023e-01\u001b[0m, \u001b[1;36m-2.1875e+00\u001b[0m, \u001b[1;36m-5.3043e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1118e+01\u001b[0m, \u001b[1;36m-2.3193e+00\u001b[0m, \u001b[1;36m-1.7046e+00\u001b[0m, \u001b[1;36m-2.4368e+00\u001b[0m, \u001b[1;36m-6.8214e-01\u001b[0m,\n",
       "          \u001b[1;36m-1.8959e+00\u001b[0m,  \u001b[1;36m1.4921e-01\u001b[0m, \u001b[1;36m-1.8533e+00\u001b[0m, \u001b[1;36m-5.1833e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.0280e+01\u001b[0m, \u001b[1;36m-2.5857e+00\u001b[0m, \u001b[1;36m-1.0144e+00\u001b[0m, \u001b[1;36m-2.3723e+00\u001b[0m, \u001b[1;36m-1.0276e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.7403e+00\u001b[0m, \u001b[1;36m-2.1357e-03\u001b[0m, \u001b[1;36m-2.1665e+00\u001b[0m,  \u001b[1;36m3.0278e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-4.8959e-01\u001b[0m, \u001b[1;36m-2.0961e+00\u001b[0m, \u001b[1;36m-6.2558e-01\u001b[0m, \u001b[1;36m-2.2961e+00\u001b[0m, \u001b[1;36m-1.3618e+00\u001b[0m,\n",
       "          \u001b[1;36m-2.0690e+00\u001b[0m, \u001b[1;36m-1.0093e+00\u001b[0m, \u001b[1;36m-1.3048e+00\u001b[0m,  \u001b[1;36m8.8662e+00\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m-1.4161e+00\u001b[0m, \u001b[1;36m-1.8178e+00\u001b[0m, \u001b[1;36m-5.6483e-01\u001b[0m, \u001b[1;36m-2.2271e+00\u001b[0m, \u001b[1;36m-1.4522e+00\u001b[0m,\n",
       "          \u001b[1;36m-2.2266e+00\u001b[0m, \u001b[1;36m-1.2002e+00\u001b[0m, \u001b[1;36m-1.2413e+00\u001b[0m,  \u001b[1;36m8.7637e+00\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1036e+01\u001b[0m, \u001b[1;36m-2.4121e+00\u001b[0m, \u001b[1;36m-1.5921e+00\u001b[0m, \u001b[1;36m-2.4863e+00\u001b[0m, \u001b[1;36m-1.3190e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.5500e+00\u001b[0m,  \u001b[1;36m6.4275e-03\u001b[0m, \u001b[1;36m-1.7939e+00\u001b[0m, \u001b[1;36m-3.4909e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1186e+01\u001b[0m, \u001b[1;36m-2.3898e+00\u001b[0m, \u001b[1;36m-1.5176e+00\u001b[0m, \u001b[1;36m-2.5681e+00\u001b[0m, \u001b[1;36m-7.8249e-01\u001b[0m,\n",
       "          \u001b[1;36m-1.8102e+00\u001b[0m, \u001b[1;36m-1.7170e-01\u001b[0m, \u001b[1;36m-1.9194e+00\u001b[0m, \u001b[1;36m-7.1405e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1479e+01\u001b[0m, \u001b[1;36m-2.3559e+00\u001b[0m, \u001b[1;36m-1.2051e+00\u001b[0m, \u001b[1;36m-2.6351e+00\u001b[0m, \u001b[1;36m-1.0897e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.8197e+00\u001b[0m, \u001b[1;36m-2.3955e-01\u001b[0m, \u001b[1;36m-1.8723e+00\u001b[0m, \u001b[1;36m-7.9155e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1569e+01\u001b[0m, \u001b[1;36m-2.4212e+00\u001b[0m, \u001b[1;36m-9.2269e-01\u001b[0m, \u001b[1;36m-2.7095e+00\u001b[0m, \u001b[1;36m-1.3123e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.9065e+00\u001b[0m, \u001b[1;36m-1.5261e-01\u001b[0m, \u001b[1;36m-1.8674e+00\u001b[0m, \u001b[1;36m-9.2102e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1516e+01\u001b[0m, \u001b[1;36m-2.3888e+00\u001b[0m, \u001b[1;36m-8.6533e-01\u001b[0m, \u001b[1;36m-2.6910e+00\u001b[0m, \u001b[1;36m-1.2451e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.9952e+00\u001b[0m, \u001b[1;36m-3.7431e-01\u001b[0m, \u001b[1;36m-1.8713e+00\u001b[0m, \u001b[1;36m-9.3357e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1226e+01\u001b[0m, \u001b[1;36m-2.3456e+00\u001b[0m, \u001b[1;36m-7.4065e-01\u001b[0m, \u001b[1;36m-2.8541e+00\u001b[0m, \u001b[1;36m-1.0089e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.9645e+00\u001b[0m,  \u001b[1;36m4.5646e-02\u001b[0m, \u001b[1;36m-1.8419e+00\u001b[0m, \u001b[1;36m-9.9125e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.6967e-01\u001b[0m, \u001b[1;36m-2.0559e+00\u001b[0m, \u001b[1;36m-7.4033e-01\u001b[0m, \u001b[1;36m-2.7475e+00\u001b[0m,  \u001b[1;36m6.3590e+00\u001b[0m,\n",
       "          \u001b[1;36m-2.4282e+00\u001b[0m,  \u001b[1;36m1.2192e-01\u001b[0m, \u001b[1;36m-2.7365e+00\u001b[0m,  \u001b[1;36m4.8693e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.2625e+00\u001b[0m, \u001b[1;36m-1.8687e+00\u001b[0m,  \u001b[1;36m4.6671e-02\u001b[0m, \u001b[1;36m-3.6221e+00\u001b[0m,  \u001b[1;36m4.0317e+00\u001b[0m,\n",
       "          \u001b[1;36m-2.2456e+00\u001b[0m,  \u001b[1;36m1.1748e+00\u001b[0m, \u001b[1;36m-2.9106e+00\u001b[0m,  \u001b[1;36m1.3734e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.0070e+01\u001b[0m, \u001b[1;36m-2.1285e+00\u001b[0m, \u001b[1;36m-1.5390e+00\u001b[0m, \u001b[1;36m-2.0444e+00\u001b[0m, \u001b[1;36m-1.5033e+00\u001b[0m,\n",
       "          \u001b[1;36m-1.8817e+00\u001b[0m, \u001b[1;36m-1.0944e+00\u001b[0m, \u001b[1;36m-1.9918e+00\u001b[0m,  \u001b[1;36m5.8473e-01\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = ner_model(sentence_input)[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 4, 4, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "predictions = torch.argmax(outputs, dim=2) \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_sentence \u001b[39m=\u001b[39m ner_model(sentence)\n\u001b[1;32m      2\u001b[0m class_sentence\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1758\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1759\u001b[0m     input_ids,\n\u001b[1;32m   1760\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1761\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1762\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1763\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1764\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1765\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1766\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1767\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1772\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:960\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    959\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 960\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m    961\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize()\n\u001b[1;32m    962\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:4201\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4198\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   4200\u001b[0m \u001b[39m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 4201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39min\u001b[39;00m input_ids[:, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m]]:\n\u001b[1;32m   4202\u001b[0m     warn_string \u001b[39m=\u001b[39m (\n\u001b[1;32m   4203\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4204\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4205\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4206\u001b[0m     )\n\u001b[1;32m   4208\u001b[0m     \u001b[39m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   4209\u001b[0m     \u001b[39m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "class_sentence = ner_model(sentence)\n",
    "class_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_classifier_model = \"vblagoje/bert-english-uncased-finetuned-pos\"  # 470 MB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aff9d1ea8114c42b53681996a6af827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a2435b26cc4b2b8b7e11c1513e0c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9b5807cba242f4a837efd41bd00f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f73ff17227a48b48d4e843aff27f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9584ee8c9a0d4faea3675afc86cf767b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parts of Speech tagging is a Token classification sub-variant\n",
    "\n",
    "pos_classifier = pipeline(\"token-classification\",\n",
    "                          model=pos_classifier_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">transformers.pipelines.token_classification.TokenClassificationPipeline</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe0859934f0</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mtransformers.pipelines.token_classification.TokenClassificationPipeline\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fe0859934f0\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pos_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'PRON',\n",
       "  'score': 0.99954295,\n",
       "  'index': 1,\n",
       "  'word': 'i',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity': 'AUX',\n",
       "  'score': 0.9976078,\n",
       "  'index': 2,\n",
       "  'word': 'am',\n",
       "  'start': 2,\n",
       "  'end': 4},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.99935,\n",
       "  'index': 3,\n",
       "  'word': 'from',\n",
       "  'start': 5,\n",
       "  'end': 9},\n",
       " {'entity': 'PROPN',\n",
       "  'score': 0.99864393,\n",
       "  'index': 4,\n",
       "  'word': 'india',\n",
       "  'start': 10,\n",
       "  'end': 15},\n",
       " {'entity': 'CCONJ',\n",
       "  'score': 0.9992494,\n",
       "  'index': 5,\n",
       "  'word': 'and',\n",
       "  'start': 16,\n",
       "  'end': 19},\n",
       " {'entity': 'PRON',\n",
       "  'score': 0.99945444,\n",
       "  'index': 6,\n",
       "  'word': 'i',\n",
       "  'start': 20,\n",
       "  'end': 21},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.99256027,\n",
       "  'index': 7,\n",
       "  'word': 'like',\n",
       "  'start': 22,\n",
       "  'end': 26},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.99910283,\n",
       "  'index': 8,\n",
       "  'word': 'playing',\n",
       "  'start': 27,\n",
       "  'end': 34},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.99935657,\n",
       "  'index': 9,\n",
       "  'word': 'with',\n",
       "  'start': 35,\n",
       "  'end': 39},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9962528,\n",
       "  'index': 10,\n",
       "  'word': 'data',\n",
       "  'start': 40,\n",
       "  'end': 44}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_classifier(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
