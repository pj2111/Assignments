{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Advanced] Write a wrapper Class that takes a dataset name, provides access to list of configs, splits, dataset info, \n",
    "# contains methods to load dataset into memory, and process it as required.  << Expect to spend atleast 60 to 90 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (get_dataset_config_info,\n",
    "                      get_dataset_config_names,\n",
    "                      get_dataset_infos,\n",
    "                      get_dataset_split_names,\n",
    "                      load_dataset,\n",
    "                      load_metric,\n",
    "                      list_datasets,\n",
    "                      list_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_wrap():\n",
    "    def __init__(self,dsname):\n",
    "        self.dsname = dsname\n",
    "        \n",
    "    def get_details(self):\n",
    "        # print(get_dataset_config_names(self.dsname))\n",
    "        d={}\n",
    "        d['config_names'] = get_dataset_config_names(self.dsname)\n",
    "        d['dataset_info'] = get_dataset_infos(self.dsname)\n",
    "        return d        \n",
    "    def ask_config(self):\n",
    "        if get_dataset_config_names(self.dsname) != ['default']:\n",
    "            self.config1= input(\"which config in this dataset do you want? \") # return self.config1\n",
    "        \n",
    "    def getconfig_details(self):\n",
    "        d={}\n",
    "        self.ask_config()\n",
    "        # config1= input(\"which config in this dataset do you want? \")\n",
    "        d['config_info'] = get_dataset_config_info(self.dsname, self.config1)         \n",
    "        d['split_names'] = get_dataset_split_names(self.dsname, self.config1)\n",
    "        return d\n",
    "    def load_ds(self):\n",
    "        self.ask_config()\n",
    "        d1=load_dataset(self.dsname,self.config1)\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=DS_wrap('glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_names': ['ax',\n",
       "  'cola',\n",
       "  'mnli',\n",
       "  'mnli_matched',\n",
       "  'mnli_mismatched',\n",
       "  'mrpc',\n",
       "  'qnli',\n",
       "  'qqp',\n",
       "  'rte',\n",
       "  'sst2',\n",
       "  'stsb',\n",
       "  'wnli'],\n",
       " 'dataset_info': {'ax': DatasetInfo(description='', citation='', homepage='', license='', features={'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='ax', version=0.0.0, splits={'test': SplitInfo(name='test', num_bytes=237694, num_examples=1104, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=80767, post_processing_size=None, dataset_size=237694, size_in_bytes=None),\n",
       "  'cola': DatasetInfo(description='', citation='', homepage='', license='', features={'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['unacceptable', 'acceptable'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='cola', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=484869, num_examples=8551, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=60322, num_examples=1043, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=60513, num_examples=1063, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=326394, post_processing_size=None, dataset_size=605704, size_in_bytes=None),\n",
       "  'mnli': DatasetInfo(description='', citation='', homepage='', license='', features={'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='mnli', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=74619646, num_examples=392702, shard_lengths=None, dataset_name=None), 'validation_matched': SplitInfo(name='validation_matched', num_bytes=1833783, num_examples=9815, shard_lengths=None, dataset_name=None), 'validation_mismatched': SplitInfo(name='validation_mismatched', num_bytes=1949231, num_examples=9832, shard_lengths=None, dataset_name=None), 'test_matched': SplitInfo(name='test_matched', num_bytes=1848654, num_examples=9796, shard_lengths=None, dataset_name=None), 'test_mismatched': SplitInfo(name='test_mismatched', num_bytes=1950703, num_examples=9847, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=57168425, post_processing_size=None, dataset_size=82202017, size_in_bytes=None),\n",
       "  'mnli_matched': DatasetInfo(description='', citation='', homepage='', license='', features={'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='mnli_matched', version=0.0.0, splits={'validation': SplitInfo(name='validation', num_bytes=1833783, num_examples=9815, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=1848654, num_examples=9796, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=2435055, post_processing_size=None, dataset_size=3682437, size_in_bytes=None),\n",
       "  'mnli_mismatched': DatasetInfo(description='', citation='', homepage='', license='', features={'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='mnli_mismatched', version=0.0.0, splits={'validation': SplitInfo(name='validation', num_bytes=1949231, num_examples=9832, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=1950703, num_examples=9847, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=2509009, post_processing_size=None, dataset_size=3899934, size_in_bytes=None),\n",
       "  'mrpc': DatasetInfo(description='', citation='', homepage='', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='mrpc', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=943843, num_examples=3668, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=105879, num_examples=408, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=442410, num_examples=1725, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=1033400, post_processing_size=None, dataset_size=1492132, size_in_bytes=None),\n",
       "  'qnli': DatasetInfo(description='', citation='', homepage='', license='', features={'question': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'not_entailment'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='qnli', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=25612443, num_examples=104743, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=1368304, num_examples=5463, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=1373093, num_examples=5463, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=19278324, post_processing_size=None, dataset_size=28353840, size_in_bytes=None),\n",
       "  'qqp': DatasetInfo(description='', citation='', homepage='', license='', features={'question1': Value(dtype='string', id=None), 'question2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_duplicate', 'duplicate'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='qqp', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=50900820, num_examples=363846, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=5653754, num_examples=40430, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=55171111, num_examples=390965, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=73982265, post_processing_size=None, dataset_size=111725685, size_in_bytes=None),\n",
       "  'rte': DatasetInfo(description='', citation='', homepage='', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'not_entailment'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='rte', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=847320, num_examples=2490, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=90728, num_examples=277, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=974053, num_examples=3000, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=1274409, post_processing_size=None, dataset_size=1912101, size_in_bytes=None),\n",
       "  'sst2': DatasetInfo(description='', citation='', homepage='', license='', features={'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='sst2', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=4681603, num_examples=67349, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=106252, num_examples=872, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=216640, num_examples=1821, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=3331080, post_processing_size=None, dataset_size=5004495, size_in_bytes=None),\n",
       "  'stsb': DatasetInfo(description='', citation='', homepage='', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': Value(dtype='float32', id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='stsb', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=754791, num_examples=5749, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=216064, num_examples=1500, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=169974, num_examples=1379, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=766983, post_processing_size=None, dataset_size=1140829, size_in_bytes=None),\n",
       "  'wnli': DatasetInfo(description='', citation='', homepage='', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_entailment', 'entailment'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='wnli', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=107109, num_examples=635, shard_lengths=None, dataset_name=None), 'validation': SplitInfo(name='validation', num_bytes=12162, num_examples=71, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=37889, num_examples=146, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=63522, post_processing_size=None, dataset_size=157160, size_in_bytes=None)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d1.get_details()    # commented as it takes 4+ min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_info': DatasetInfo(description='', citation='', homepage='', license='', features={'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='glue', config_name='ax', version=0.0.0, splits={'test': SplitInfo(name='test', num_bytes=237694, num_examples=1104, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=80767, post_processing_size=None, dataset_size=237694, size_in_bytes=None),\n",
       " 'split_names': ['test']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.getconfig_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 1.23M/1.23M [00:02<00:00, 431kB/s]\n",
      "Downloading data: 100%|██████████| 312k/312k [00:00<00:00, 445kB/s]\n",
      "Downloading data: 100%|██████████| 283k/283k [00:00<00:00, 302kB/s]\n",
      "Generating train split: 100%|██████████| 14041/14041 [00:00<00:00, 91909.95 examples/s]\n",
      "Generating validation split: 100%|██████████| 3250/3250 [00:00<00:00, 70073.24 examples/s]\n",
      "Generating test split: 100%|██████████| 3453/3453 [00:00<00:00, 80182.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### Practice mapping function for preprocessing datasets\n",
    "\n",
    "import datasets\n",
    "conll2003 = datasets.load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tokens'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'EU'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rejects'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'German'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'call'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'boycott'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'British'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lamb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Peter'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Blackburn'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'BRUSSELS'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1996-08-22'</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pos_tags'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_tags'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">]]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ner_tags'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'id'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[32m'1'\u001b[0m, \u001b[32m'2'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'tokens'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m[\u001b[0m\u001b[32m'EU'\u001b[0m, \u001b[32m'rejects'\u001b[0m, \u001b[32m'German'\u001b[0m, \u001b[32m'call'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'boycott'\u001b[0m, \u001b[32m'British'\u001b[0m, \u001b[32m'lamb'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[32m'Peter'\u001b[0m, \u001b[32m'Blackburn'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[32m'BRUSSELS'\u001b[0m, \u001b[32m'1996-08-22'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'pos_tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m22\u001b[0m, \u001b[1;36m42\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m37\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m7\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m22\u001b[0m, \u001b[1;36m22\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m22\u001b[0m, \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'chunk_tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m11\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m22\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'ner_tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "print(conll2003['train'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "# tokenizer1 = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dialogue'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14732</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dialogue'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">819</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    validation: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dialogue'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">818</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'dialogue'\u001b[0m, \u001b[32m'summary'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m14732\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'dialogue'\u001b[0m, \u001b[32m'summary'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m819\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    validation: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'dialogue'\u001b[0m, \u001b[32m'summary'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m818\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from rich import print\n",
    "\n",
    "dataset_samsum = datasets.load_dataset('samsum')\n",
    "print(dataset_samsum)#['train'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.0/404.0 kB\u001b[0m \u001b[31m545.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.26.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_ckpt = 'google/pegasus-cnn_dailymail'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [109, 1], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examples_to_feat(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "        \n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask' : input_encodings['attention_mask'],\n",
    "        'labels' : target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 14732/14732 [00:19<00:00, 747.04 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 989.44 examples/s] \n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 1050.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dialogue'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14732</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dialogue'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">819</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    validation: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dialogue'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">818</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'dialogue'\u001b[0m, \u001b[32m'summary'\u001b[0m, \u001b[32m'input_ids'\u001b[0m, \u001b[32m'attention_mask'\u001b[0m, \u001b[32m'labels'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m14732\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'dialogue'\u001b[0m, \u001b[32m'summary'\u001b[0m, \u001b[32m'input_ids'\u001b[0m, \u001b[32m'attention_mask'\u001b[0m, \u001b[32m'labels'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m819\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    validation: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'dialogue'\u001b[0m, \u001b[32m'summary'\u001b[0m, \u001b[32m'input_ids'\u001b[0m, \u001b[32m'attention_mask'\u001b[0m, \u001b[32m'labels'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m818\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(examples_to_feat, batched=True)\n",
    "print(dataset_samsum_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_samsum_pt --> without , batched=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1659083</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    validation: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">520</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2507</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'translation'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m1659083\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    validation: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'translation'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m520\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'translation'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m2507\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Give your application an accessibility workout'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hi'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Accerciser Accessibility Explorer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hi'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'एक्सेर्साइसर पहुंचनीयता अन्वेषक'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The default plugin layout for the bottom panel'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hi'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'निचले पटल के लिए डिफोल्ट प्लग-इन खाका'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'translation'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'en'\u001b[0m: \u001b[32m'Give your application an accessibility workout'\u001b[0m, \u001b[32m'hi'\u001b[0m: \u001b[32m'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'en'\u001b[0m: \u001b[32m'Accerciser Accessibility Explorer'\u001b[0m, \u001b[32m'hi'\u001b[0m: \u001b[32m'एक्सेर्साइसर पहुंचनीयता अन्वेषक'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'en'\u001b[0m: \u001b[32m'The default plugin layout for the bottom panel'\u001b[0m, \u001b[32m'hi'\u001b[0m: \u001b[32m'निचले पटल के लिए डिफोल्ट प्लग-इन खाका'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "print(raw_datasets)\n",
    "print(raw_datasets['train'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26618</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16155</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">346</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33383</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m26618\u001b[0m, \u001b[1;36m16155\u001b[0m, \u001b[1;36m346\u001b[0m, \u001b[1;36m33383\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   8%|▊         | 135000/1659083 [00:38<13:54, 1825.94 examples/s]"
     ]
    }
   ],
   "source": [
    "max_input_len = 128\n",
    "max_target_len = 128\n",
    "\n",
    "source_lang = 'en'\n",
    "target_lang = 'hi'\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples['translation']]\n",
    "    targets = [ex[target_lang] for ex in examples['translation']]\n",
    "    model_inputs = tokenizer(inputs, max_length = max_input_len, truncation = True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length = max_target_len, truncation = True)\n",
    "        \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">contextlib._GeneratorContextManager</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fde59859130</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mcontextlib._GeneratorContextManager\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fde59859130\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(tokenizer)\n",
    "print(tokenizer.as_target_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(arg1, arg2, *args, **kwargs):\n",
    "# Now make a function that takes text, along with other \n",
    "# args, and returns a sentiment label\n",
    "    pass\n",
    "\n",
    "#logging\n",
    "#timimg\n",
    "\n",
    "text, dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class senti takes in all paraemters trains a sentiment classificatuin model and gives out classsification label\n",
    "# modify it to take in a list of n reviews or just one review and provide label for each review\n",
    "# use logging\n",
    "class Senti():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def tokenizing(text):\n",
    "        pass\n",
    "    def model_loading():\n",
    "        pass\n",
    "    def classify(self.text):\n",
    "        tokenizing(self.text)\n",
    "        model_loading()      # only once not always\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/KamalRaj/Assignments/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "logging.basicConfig(filename='log5.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "def make_pipeline(text,model='distilbert-base-uncased'):\n",
    "    classifier=pipeline(\"sentiment-analysis\", model=model)\n",
    "    logging.info(\"classifier created\")\n",
    "    return classifier(text)[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pipeline(\"I loved Star Wars so much!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pipeline(\"xyz is the worst movie ever!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline(\"sentiment-analysis\", model='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5267055630683899}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I loved Star Wars so much!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
